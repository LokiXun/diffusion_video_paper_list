# StyleTex: Style Image-Guided Texture Generation for 3D Models

> "StyleTex: Style Image-Guided Texture Generation for 3D Models" SIGGRAPH, 2024 Nov 1
> [paper](http://arxiv.org/abs/2411.00399v1) [code]() [pdf](./2024_11_SIGGRAPH_StyleTex--Style-Image-Guided-Texture-Generation-for-3D-Models.pdf) [note](./2024_11_SIGGRAPH_StyleTex--Style-Image-Guided-Texture-Generation-for-3D-Models_Note.md)
> Authors: Zhiyu Xie, Yuqing Zhang, Xiangjun Tang, Yiqian Wu, Dehan Chen, Gongsheng Li, Xaogang Jin

## Key-point

- Task
- Problems
- :label: Label:

## Contributions

## Introduction

![fig1](docs/2024_11_SIGGRAPH_StyleTex--Style-Image-Guided-Texture-Generation-for-3D-Models_Note/fig1.png)



### Text/Image-guided Texture Generation

ä½¿ç”¨ SDS Loss è’¸é¦ 2D diffusion prior

> When it comes to text-to-3D, numerous approaches have been developed to optimize 3D representations by distilling 2D diffusion models, using techniques like score distillation sampling (SDS) [Poole et al. 2022]. 

> At each iteration, the differentiable rendering function ð‘” renders the trainable paramaters ðœƒ from camera ð‘, getting the rendered image ð‘¥0.

![eq1](docs/2024_11_SIGGRAPH_StyleTex--Style-Image-Guided-Texture-Generation-for-3D-Models_Note/eq1.png)





## methods

![fig2](docs/2024_11_SIGGRAPH_StyleTex--Style-Image-Guided-Texture-Generation-for-3D-Models_Note/fig2.png)

è®¾è®¡ ODCR æ¨¡å—ï¼Œä»Ž reference image æå– style ç‰¹å¾





### Style Score Distribution

> To achieve the style distribution for ðœ–ð‘ ð‘¡ ð‘¦ð‘™ð‘’ , a possible way is to train a style-conditioned diffusion model, but it is time-consuming. 

ä¹‹å‰å·¥ä½œå‘çŽ° UNet Cross-attn layer ä¸åŒå±‚è´Ÿè´£ä¸åŒç‰¹å¾çš„ç”Ÿæˆ :star:

> Existing 2D style image generation studies [Wang et al. 2024; Ye et al. 2023] have explored that the cross-attention mechanism in different transformer layers of a diffusion model exerts different effects on the content and style. Therefore, the stylized result can be achieved by injecting the features of the reference image into the layers that are responsible for style effects.
>
> - "InstantStyle: Free Lunch towards Style-Preserving in Text-to-Image Generation" Arxiv, 2024 Apr 3
>   [paper](http://arxiv.org/abs/2404.02733v2) [code](https://github.com/InstantStyle/InstantStyle.) [pdf](./2024_04_Arxiv_InstantStyle--Free-Lunch-towards-Style-Preserving-in-Text-to-Image-Generation.pdf) [note](./2024_04_Arxiv_InstantStyle--Free-Lunch-towards-Style-Preserving-in-Text-to-Image-Generation_Note.md)
>   Authors: Haofan Wang, Matteo Spinelli, Qixun Wang, Xu Bai, Zekui Qin, Anthony Chen



å› æ­¤å¯¹äºŽè´Ÿè´£ style çš„ cross-attn layerï¼ŒæŠŠ reference å›¾çš„ç‰¹å¾æ³¨å…¥

> Leveraging such a layer to inject the reference image feature may introduce unintended content, while ignoring it may result in inaccuracies in style expressiveness, such as color tone shifting. To address this, we aim to incorporate as many layers that are responsible for style effects as possible to maintain style expressiveness. 



è®¾è®¡ä¸€ä¸ªæ­£äº¤åˆ†è§£æŠŠ content ç»™åŽ»æŽ‰ï¼Œæå– style ç‰¹å¾ :star:

> Specifically, we employ an orthogonal decomposition for content removal (ODCR)

![eq9](docs/2024_11_SIGGRAPH_StyleTex--Style-Image-Guided-Texture-Generation-for-3D-Models_Note/eq9.png)



## setting

- **An NVIDIA RTX 4090 GPU** is used for the optimization process, which takes about 15 minutes to synthesize a texture map for each mesh

## Experiment

> ablation study çœ‹é‚£ä¸ªæ¨¡å—æœ‰æ•ˆï¼Œæ€»ç»“ä¸€ä¸‹

çœ‹ç”Ÿæˆçš„çº¹ç†æ˜¯å¦ content-free åªæœ‰é£Žæ ¼

![fig8](docs/2024_11_SIGGRAPH_StyleTex--Style-Image-Guided-Texture-Generation-for-3D-Models_Note/fig8.png)



## Limitations

## Summary :star2:

> learn what

### how to apply to our task

