# High-Resolution Image Synthesis with Latent Diffusion Models

> "High-Resolution Image Synthesis with Latent Diffusion Models" CVPR-highlight, 2021 Dec 20
> [paper](http://arxiv.org/abs/2112.10752v2) [code](https://github.com/CompVis/latent-diffusion) [SDv2.1-cdoe](https://github.com/Stability-AI/stablediffusion) [pdf](./2021_12_CVPR-highlight_High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models.pdf) [note](./2021_12_CVPR-highlight_High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models_Note.md)
> Authors: Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, BjÃ¶rn Ommer
>
> [Stable Diffusion with ðŸ§¨ Diffusers](https://huggingface.co/blog/stable_diffusion#how-does-stable-diffusion-work)

## Key-point

- Task: Image Synthesis

- Problems

  - RGB pixel space æŽ¨ç†æ¶ˆè€—æ˜¾å­˜

    > However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations

- :label: Label:

## Contributions

- ä½¿ç”¨ VAE æ˜ å°„åˆ° latent space å†åŽ»ç”Ÿæˆï¼Œé™ä½Žæ˜¾å­˜ï¼ŒåŒæ—¶ä¿è¯é‡å»ºè´¨é‡

> To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders
>
> In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity.

- ä½¿ç”¨ cross-attn æ–¹å¼å¼•å…¥æ›´å¤š conditionï¼Œæ”¯æŒæ–‡æœ¬å¼•å¯¼

> By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner

- å¤šä¸ª task SOTA åŒæ—¶æ˜¾å­˜è¦æ±‚æ›´ä½Ž

> Our latent diffusion models (LDMs) achieve new state-of-the-art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including text-to-image synthesis, unconditional image generation and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.
>
> We achieve competitive performance on multiple tasks **(unconditional image synthesis, inpainting, stochastic super-resolution)** and datasets while significantly lowering computational costs. Compared to pixel-based diffusion approaches, we also significantly decrease inference costs

- ç›¸æ¯” VAE & Diffusion åŒæ—¶è®­ç»ƒçš„æ¨¡åž‹ï¼Œä¸éœ€è¦è€ƒè™‘ weighting -> é‡å»º & ç”Ÿæˆè´¨é‡ï¼Œè®­ç»ƒç®€å•ä¸€äº›

> We show that, in contrast to previous work [93] which learns both an encoder/decoder architecture and a score-based prior simultaneously, our approach does not require a delicate weighting of reconstruction and generative abilities. This ensures extremely faithful reconstructions and requires very little regularization of the latent space





## Introduction

1. GAN æ–¹æ³•ç”Ÿæˆæ ·æœ¬å¤šæ ·æ€§ä¸è¶³ & è®­ç»ƒä¸ç¨³å®š
2. VAE æ–¹æ³•ç”Ÿæˆï¼Œç”Ÿæˆå›¾åƒçš„è´¨é‡ä¸è¡Œ
3. DDPM åœ¨ RGB åƒç´ ç©ºé—´ç”Ÿæˆï¼Œç”Ÿæˆæ…¢

> image generation has been tackled mainly through **four families of models**: Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), Autoregressive Models (ARMs), Diffusion Probabilistic Models(DMs)
>
> - GANs: show promising results for **data with limited variability**, bearing mode-collapse, **unstable training**
>
> **Mode collapse**: *this phenomenon occurs when the generator can alternately generate a limited number of outputs that fool the discriminator. In general, GANs **struggle capturing the full data distribution.***
>
> - VAE(Variational Autoencoders)
>
> not suffer from mode-collapse and can efficiently generate high-resolution images. **sample quality** is **not** always **comparable to** that of **GANs**.
>
> - DDPM
>
> operation in **pixel space** by adding or removing noise to a tensor of the same size as the original image results in **slow inference speed** and **high computational cost**





åŸºäºŽ GAN çš„æ–¹æ³•ï¼Œå¤šæ•°åœ¨å°‘é‡æ•°æ®ä¸Šé€šè¿‡å¯¹æŠ—å­¦ä¹ çš„æ–¹å¼è®­ç»ƒ & å­˜åœ¨è®­ç»ƒä¸ç¨³å®š & å­˜åœ¨ mode-collapse é—®é¢˜ï¼Œå¾ˆéš¾è®­ç»ƒå‚æ•°é‡æå‡åˆ° billion çº§åˆ«çš„æ¨¡åž‹

> In contrast, the promising results of GANs [3, 27, 40] have been revealed to be mostly confined to data with comparably limited variability as their adversarial learning procedure does not easily scale to modeling complex, multi-modal distributions

- Qï¼šä»€ä¹ˆæ˜¯ GAN æ–¹æ³•è®­ç»ƒçš„ mode collapse é—®é¢˜ï¼Ÿ

ç”Ÿæˆå™¨åªç”Ÿæˆå›ºå®šçš„å‡ ä¸ªæ ·æœ¬ï¼Œå…¶ä»–éƒ½å¾ˆåžƒåœ¾ï¼Œæ²¡æ³•å»ºæ¨¡å®Œæ•´çš„æ•°æ®åˆ†å¸ƒ

> **Mode collapse**: *this phenomenon occurs when the generator can alternately generate a limited number of outputs that fool the discriminator. In general, GANs **struggle capturing the full data distribution.***





DDPM ç­‰æ–¹æ³•å±•ç¤ºå‡ºå¾ˆå¥½çš„å›¾åƒç”Ÿæˆæ€§èƒ½ï¼Œä¹Ÿæœ‰ä¸€äº› SRï¼Œclass-condition çš„å·¥ä½œï¼Œå¯¹äºŽä¸åŒä»»åŠ¡çš„é€‚é…æ€§èƒ½å¾ˆå¥½

> Recently, diffusion models [82], which are built from a hierarchy of denoising autoencoders, have shown to achieve impressive results in image synthesis [30,85] and beyond [7,45,48,57], and define the state-of-the-art in class-conditional image synthesis [15,31] and super-resolution [72]. Moreover, even unconditional DMs can readily be applied to tasks such as inpainting and colorization [85] or stroke-based synthesis [53], in contrast to other types of generative models [19,46,69].

likelihood-based model é¿å…äº† GAN æ–¹æ³•çš„ mode-collapse é—®é¢˜ï¼Œè®­ç»ƒæ›´ç¨³å®šã€‚å¤š timestep ä½¿ç”¨ä¸€ä¸ªæ¨¡åž‹ï¼Œèƒ½å¤Ÿå‡æ¨¡æ›´å¤æ‚çš„æ•°æ®åˆ†å¸ƒã€‚èƒ½å¤Ÿæ›´æ–¹ä¾¿åœ°è®­ç»ƒ billion å‚æ•°é‡çš„æ¨¡åž‹

> Being likelihood-based models, they do not exhibit **mode-collapse and training instabilities as GANs** and, by heavily exploiting parameter sharing, they can model highly complex distributions of natural images without involving billions of parameters as in AR models [67].





é«˜åˆ†è¾¨çŽ‡å›¾åƒç”Ÿæˆï¼ŒDDPM ç”Ÿæˆå ç”¨æ˜¾å­˜å¾ˆå¤š

>  Although the reweighted variational objective [30] aims to address this by undersampling the initial denoising steps, DMs are still computationally demanding, since training and evaluating such a model requires repeated function evaluations (and gradient computations) in the high-dimensional space of RGB images.

- Qï¼šè®­ç»ƒæˆæœ¬ï¼Ÿ

"Diffusion models beat gan" æ–¹æ³•è®­ç»ƒéœ€è¦ 1k V100 Days, 8 å¡å°±æ˜¯ 125 å¤©

æŽ¨ç†è¦ 1k step ä¹Ÿè´¼æ…¢ï¼Œä¸€ä¸ª V100 åŽ»ç”Ÿæˆ 50k ä¸ªå›¾åƒéœ€è¦ 5å¤©



> As an example, training the most powerful DMs often takes hundreds of GPU days (e.g. **150 - 1000 V100 days in [15])** and repeated evaluations on a noisy version of the input space render also inference expensive, so that producing **50k samples takes approximately 5 days [15] on a single A100 GPU.** 
>
> - "Diffusion models beat gans on image synthesis" COPR, 2021 https://arxiv.org/pdf/2105.05233





- Qï¼šmotivation?

è®­ç»ƒè¦æ±‚é«˜ï¼Œåªæœ‰å°‘æ•°å¡å¤šçš„ç»„å¯ä»¥åš & è®­ç»ƒæˆæœ¬é«˜ -> ç¢³æŽ’æ”¾ã€‚ã€‚ã€‚

> This has two consequences for the research community and users in general: Firstly, training such a model requires massive computational resources only available to a small fraction of the field, and leaves a huge carbon footprint [65, 86]. 

æŽ¨ç†æ—¶é—´ & æ˜¾å­˜å¤§

> Secondly, evaluating an already trained model is also expensive in time and memory, since the same model architecture must run sequentially for a large number of steps (e.g. 25 - 1000 steps in [15]).

éœ€è¦ä¸€ç§æ˜¾å­˜è¦æ±‚æ›´ä½Žçš„æ–¹æ³•ï¼ŒåŠ é€ŸæŽ¨ç†ï¼Œè®©æ¨¡åž‹æ›´å¤šåœ°è¢«ç”¨èµ·æ¥

> **To increase the accessibility of this powerful model class and at the same time reduce its significant resource consumption**, a method is needed that reduces the computational complexity for both training and sampling. Reducing the computational demands of DMs without impairing their performance is, therefore, key to enhance their accessibility







- Qï¼šè½¬åˆ° latent åˆç†å—ï¼Ÿ

è®­ç»ƒä¸€ä¸ªä¸åŒåŽ‹ç¼©æ¯”çš„æ¨¡åž‹ï¼Œé‡å»ºä¸€ä¸‹åŽ»è®¡ç®— RMSEã€‚

åŽ‹ç¼©çš„è¶Šå¤šï¼Œç»†èŠ‚æŸå¤±è¶Šå¤šã€‚å‘çŽ°å¤§å¤šæ•° bit æ˜¯ç²—çœ‹æ„ŸçŸ¥ä¸åˆ°çš„ç»†èŠ‚ï¼Œè¿™éƒ¨åˆ†ç”Ÿæˆè®¤ä¸ºå¯ä»¥ä¼˜åŒ–æŽ‰

> : Fig. 2 shows the rate-distortion trade-off of a trained model. 

![fig2](docs/2021_12_CVPR-highlight_High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models_Note/fig2.png)



è®­ç»ƒ 2 ä¸ª stage

1.  perceptual compression stageï¼šè®­ç»ƒä¸€ä¸ª VAEï¼ŒåŽ»æŽ‰é«˜é¢‘ç»†èŠ‚
2. è®­ç»ƒ diffusion ç”Ÿæˆæ¨¡åž‹

> As with any likelihood-based model, learning can be roughly divided into two stages: First is a perceptual compression stage which removes high-frequency details but still learns little semantic variation. 
>
>  In the second stage, the actual generative model learns the semantic and conceptual composition of the data (semantic compression).

ç›®çš„æ˜¯æ‰¾åˆ°ä¸€ä¸ªç”Ÿæˆå›¾åƒçš„æ„ŸçŸ¥å·®ä¸å¤š & è®¡ç®—é‡æ›´å°‘çš„ space

> We thus aim to first find a perceptually equivalent, but computationally more suitable space, in which we will train diffusion models for high-resolution image synthesis





è®­ç»ƒ VAEï¼Œç¡®å®š low-dimensional spaceï¼Œä¹Ÿä¸ç”¨åŽ‹ç¼©å¤ªå¤š -> Latent space 

> Following common practice [11, 23, 66, 67, 96], we separate training into two distinct phases: **First, we train an autoencoder which provides a lower-dimensional (and thereby efficient) representational space** which is perceptually equivalent to the data space.
>
> Importantly, and in contrast to previous work [23,66], **we do not need to rely on excessive spatial compression, as we train DMs in the learned latent space**, which exhibits better scaling properties with respect to the spatial dimensionality.

Latent space ä¸ŠåŽ»å­¦ä¹  diffusion modelï¼Œè¿›è¡Œç”Ÿæˆï¼Œç§°ä½œ LDM

> The reduced complexity also provides efficient **image generation from the latent space with a single network pass**. We dub the resulting model class **Latent Diffusion Models (LDMs).**

åŒæ—¶ VAE é‡å»ºè´¨é‡å¯¹æ¯” GAN æ–¹æ³•å¥½ä¸€ä¸¢

![fig1](docs/2021_12_CVPR-highlight_High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models_Note/fig1.png)





- Qï¼šå¥½å¤„ï¼Ÿ

latent space è®­å®Œä¸€æ¬¡ä»¥åŽï¼Œå¯ä»¥é€‚é…åŽç»­ä¸åŒä»»åŠ¡

> A notable advantage of this approach is that we need to train the universal autoencoding stage only once and can therefore reuse it for multiple DM trainings or to explore possibly completely different tasks [81].
>
> This enables efficient exploration of a large number of diffusion models for various image-to-image and text-to-image tasks

Diffusion ç½‘ç»œç»“æž„ç”¨ UNet

> For the latter, we design an architecture that connects transformers to the DMâ€™s UNet backbone [71] and enables arbitrary types of token-based conditioning mechanisms, see Sec. 3.3
>
> - "Unet: Convolutional networks for biomedical image segmentation" MICCAI, 2015  https://arxiv.org/abs/1505.04597

![unet_fig1](docs/2021_12_CVPR-highlight_High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models_Note/unet_fig1.png)



### Two-Stage Image Synthesis

> VQ-VAEs [67, 101] use autoregressive models to learn an expressive prior over a discretized latent space. 
>
> - "Generating Diverse High-Fidelity Images with VQ-VAE-2" NIPS, 2019 Jun 2
>   [paper](http://arxiv.org/abs/1906.00446v1) [code]() [pdf](./2019_06_NIPS_Generating-Diverse-High-Fidelity-Images-with-VQ-VAE-2.pdf) [note](./2019_06_NIPS_Generating-Diverse-High-Fidelity-Images-with-VQ-VAE-2_Note.md)
>   Authors: Ali Razavi, Aaron van den Oord, Oriol Vinyals
>
> - "Videogpt: Video generation using VQ-VAE and transformers" CoPR, https://arxiv.org/abs/2104.10157
>
> - "Taming Transformers for High-Resolution Image Synthesis" CVPR, 2020 Dec, **VQ-GAN** :star:
>
>   [paper](https://arxiv.org/abs/2012.09841) [website](https://compvis.github.io/taming-transformers/) [note](./2021_CVPR_VQGAN_Taming-Transformers-for-High-Resolution-Image-Synthesis_Note.md)



VQ-VAEs Encoder å…ˆæå–ç‰¹å¾ -> å†æ˜ å°„åˆ°ä¸€ä¸ªç¦»æ•£çš„ codebook

> The VQ-VAE model [37] can be better **understood as a communication system.** It comprises of an encoder that maps observations onto a sequence of discrete latent variables, and a decoder that reconstructs the observations from these discrete variables. 
>
> - "Generating Diverse High-Fidelity Images with VQ-VAE-2" NIPS, 2019 Jun 2
>   [paper](http://arxiv.org/abs/1906.00446v1) [code]() [pdf](./2019_06_NIPS_Generating-Diverse-High-Fidelity-Images-with-VQ-VAE-2.pdf) [note](./2019_06_NIPS_Generating-Diverse-High-Fidelity-Images-with-VQ-VAE-2_Note.md)
>   Authors: Ali Razavi, Aaron van den Oord, Oriol Vinyals

![eq1](docs/2019_06_NIPS_Generating-Diverse-High-Fidelity-Images-with-VQ-VAE-2_Note/eq1.png)

VQ-GAN ç”¨äºŽç”Ÿæˆ

> Different from VQ-VAEs, VQGANs [23, 103] employ a first stage with an adversarial and perceptual objective to scale autoregressive transformers to larger images

![VQGAN_approach_overview.jpg](docs/2021_12_CVPR-highlight_High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models_Note/VQGAN_approach_overview.jpg)

> Our work prevents such tradeoffs, as our proposed LDMs scale more gently to higher dimensional latent spaces due to their convolutional backbone. Thus, we are free to choose the level of compression which optimally mediates between learning a powerful first stage, without leaving too much perceptual compression up to the generative diffusion model while guaranteeing highfidelity reconstructions (see Fig. 1).





## methods

![fig3](docs/2021_12_CVPR-highlight_High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models_Note/fig3.png)



### Perceptual Image Compression

VAE çš„è®­ç»ƒï¼Œå‚è€ƒ VQ-GAN

1. perceptual loss
2. patch-based [33] adversarial objective

> Our perceptual compression model is based on previous work [23] and consists of an autoencoder trained by combination of a perceptual loss [106] and a patch-based [33] adversarial objective [20, 23, 103]. 
>
> - "Taming Transformers for High-Resolution Image Synthesis" CVPR, 2020 Dec, **VQ-GAN** :star:
>
>   [paper](https://arxiv.org/abs/2012.09841) [website](https://compvis.github.io/taming-transformers/) [note](./2021_CVPR_VQGAN_Taming-Transformers-for-High-Resolution-Image-Synthesis_Note.md)

![VQGAN_approach_overview.jpg](docs/2021_12_CVPR-highlight_High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models_Note/VQGAN_approach_overview.jpg)

- Qï¼šç”¨è¿™ä¸¤ä¸ª loss çš„å¥½å¤„ï¼Ÿ

ä½¿ç”¨ L1 loss ä¼šå¯¼è‡´å±€éƒ¨åŒºåŸŸçš„ç»†èŠ‚ä¸¢å¤±ï¼Œé€ æˆ blur

> This ensures that the reconstructions are confined to the image manifold by enforcing local realism and avoids bluriness introduced by relying solely on pixel-space losses such as L2 or L1 objectives.





å‡ ä¸ªç¬¦å·ï¼Œx ä¸º RGB å›¾åƒ

![VAE-symbol](docs/2021_12_CVPR-highlight_High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models_Note/VAE-symbol.png)





é¿å…è®­ç»ƒä¸€ä¸ªéšæœºçš„é«˜ç»´åº¦ latent space -> å¢žåŠ ä¸€äº›æ­£åˆ™åŒ–é™åˆ¶

> In order to avoid arbitrarily high-variance latent spaces, we experiment with two different kinds of regularizations.

loss ç±»ä¼¼ VQ-VAE

![eq2](docs/2019_06_NIPS_Generating-Diverse-High-Fidelity-Images-with-VQ-VAE-2_Note/eq2.png)

> The first variant, KL-reg., imposes a slight KL-penalty towards a standard normal on the learned latent, similar to a VAE [46, 69], whereas VQ-reg. uses a vector quantization layer [96] within the decoder. This model can be interpreted as a VQGAN [23] but with the quantization layer absorbed by the decoder. 

ä½¿ç”¨ 2ç»´åº¦ h,w çš„ latentï¼Œä¹‹å‰çš„ codebook ä½¿ç”¨ 1D åºåˆ—ï¼Œå¿½ç•¥äº†å›¾åƒçš„ç»“æž„ã€‚ä½¿ç”¨ 2d latent è¿˜åŽŸç»†èŠ‚æ›´å¥½

>  This is in contrast to previous works [23, 66], which relied on an arbitrary 1D ordering of the learned space z to model its distribution autoregressively and thereby ignored much of the inherent structure of z.
>
> Hence, our compression model preserves details of x better (see Tab. 8). The full objective and training details can be found in the supplement.



### Latent Diffusion Models

Diffusion åŽŸç†ç±»ä¼¼

> - "Understanding Diffusion Models: A Unified Perspective" Arxiv, 2022 Aug 25
>   [paper](http://arxiv.org/abs/2208.11970v1) [code]() [pdf](./2022_08_Arxiv_Understanding-Diffusion-Models--A-Unified-Perspective.pdf) [note](./2022_08_Arxiv_Understanding-Diffusion-Models--A-Unified-Perspective_Note.md)
>   Authors: Calvin Luo

![eq1](docs/2021_12_CVPR-highlight_High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models_Note/eq1.png)

VAE è®­ç»ƒçš„ latent space é‡å»ºèƒ½åŠ›å¯ä»¥ï¼Œè¿˜è¿›è¡Œäº†åŽ‹ç¼©ã€‚åŒæ ·é€‚ç”¨ loglikelihood loss

> Compared to the high-dimensional pixel space, this space is more suitable for likelihood-based generative models, as they can now (i) focus on the important, semantic bits of the data and (ii) train in a lower dimensional, computationally much more efficient space.





- Q: UNet ç»“æž„ï¼Ÿ

time condition UNet

> The neural backbone  $\epsilon$Î¸(â—¦, t) of our model is realized as a **time-conditional UNet [71].**
>
> [StableDiffusion_structure.drawio](./StableDiffusion_structure.drawio)





### Conditioning Mechanisms

å½“æ—¶ condition å¤šæ•°éƒ½æ˜¯ class label & blurred å›¾åƒï¼Œä½¿ç”¨æ–‡ç”Ÿå›¾çš„è¿˜å¾ˆå°‘

> In the context of image synthesis, however, combining the generative power of DMs with other types of conditionings beyond **class-labels [15] or blurred variants of the input image** [72] is so far an under-explored area of research.

åœ¨ UNet å†…ç”¨ cross-attn å¼•å…¥æ–‡æœ¬

> We turn DMs into more flexible conditional image generators by augmenting their underlying UNet backbone with the cross-attention mechanism [97], which is effective for learning attention-based models of various input modalities [35,36]. 

![attn-eq](docs/2021_12_CVPR-highlight_High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models_Note/attn-eq.png)

è¿˜æ˜¯ç”¨åŽŸæ¥çš„ loss ä¸€èµ·è®­ç»ƒå°± ok

![eq3](docs/2021_12_CVPR-highlight_High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models_Note/eq3.png)







æ¯ä¸ª word å¯¹åº” token æœ‰ç‚¹æ„ä¹‰çš„

![fig4](docs/2024_07_ECCV_AccDiffusion--An-Accurate-Method-for-Higher-Resolution-Image-Generation_Note/fig4.png)





## setting

- Exp é‡Œé¢è¯´ä¸€äº›æ¨¡åž‹ç”¨ä¸€ä¸ª A100 è®­ç»ƒçš„



### SD Version

SDv1.5 åœ¨ SDv1.2 åŸºç¡€ä¸Šç”¨ laion-aesthetics v2 5+ æ•°æ®é›†å¾®è°ƒ + æå‡ CFG

> https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5
>
> The **Stable-Diffusion-v1-5** checkpoint was initialized with the weights of the [Stable-Diffusion-v1-2](https://huggingface.co/steps/huggingface.co/CompVis/stable-diffusion-v1-2) checkpoint and subsequently fine-tuned on 595k steps at resolution 512x512 on "laion-aesthetics v2 5+" and 10% dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).

SDv2  æ”¯æŒ 768x768ï¼Œ UNet ä¸€æ ·ï¼ŒTextCLIP æ›¿æ¢ä¸º OpenCLIP-ViT/Hï¼Œä½¿ç”¨ v-prediction loss

> https://github.com/Stability-AI/stablediffusion :star:
>
> - New stable diffusion model (*Stable Diffusion 2.0-v*) at 768x768 resolution. Same number of parameters in the U-Net as 1.5, but uses [OpenCLIP-ViT/H](https://github.com/mlfoundations/open_clip) as the text encoder and is trained from scratch. *SD 2.0-v* is a so-called [v-prediction](https://arxiv.org/abs/2202.00512) model.

> - https://huggingface.co/stabilityai/stable-diffusion-2

SDv2.1 è°ƒæ•´å‚æ•°ç»§ç»­å¾®è°ƒ 55k + 155k steps

> - https://huggingface.co/stabilityai/stable-diffusion-2-1
>
> This `stable-diffusion-2-1` model is fine-tuned from [stable-diffusion-2](https://huggingface.co/stabilityai/stable-diffusion-2) (`768-v-ema.ckpt`) with an additional 55k steps on the same dataset (with `punsafe=0.1`), and then fine-tuned for another 155k extra steps with `punsafe=0.98`.







## Experiment

> ablation study çœ‹é‚£ä¸ªæ¨¡å—æœ‰æ•ˆï¼Œæ€»ç»“ä¸€ä¸‹

>  A visual comparison between the effects of first stage regularization schemes on LDM training and their generalization abilities to resolutions > 2562 can be found in Appendix D.1.

ç½‘ç»œç»“æž„ç»†èŠ‚ï¼Œçœ‹  E.2

>  In E.2 we list details on architecture, implementation, training and evaluation for all results presented in this section.



Efficiency Analysis

> All models have a comparable number of parameters as provided in Tab. 13 and 14. We maximize the learning rates of the individual models such that they still train stably. Therefore, the learning rates slightly vary between different runs cf . Tab. 13 and 14.

![tb13](docs/2021_12_CVPR-highlight_High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models_Note/tb13.png)

![tb14](docs/2021_12_CVPR-highlight_High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models_Note/tb14.png)





###  On Perceptual Compression Tradeoffs

å¯¹æ¯”ä¸åŒåŽ‹ç¼©æ¯” f è®­ç»ƒå‡ºæ¥çš„æ•ˆæžœ

> This section analyzes the behavior of our LDMs with different downsampling factors f âˆˆ {1, 2, 4, 8, 16, 32}

éƒ½ç”¨ä¸€ä¸ª A100 è®­ç»ƒ

> To obtain a comparable test-field, we fix the computational resources to a single NVIDIA A100 for all experiments in this section and train all models for the same number of steps and with the same number of parameters.







è™½ç„¶ VQ-VAE çš„é‡å»ºç¨å¾®çƒ‚ä¸€ä¸¢ä¸¢ï¼Œä½†è®­ç»ƒçš„ LDM ç”Ÿæˆè´¨é‡å±…ç„¶æ›´å¥½

> . Interestingly, we find that LDMs trained in VQregularized latent spaces sometimes achieve better sample quality, **even though the reconstruction capabilities of VQregularized first stage models slightly fall behind those of their continuous counterparts**, cf . Tab. 8.

çœ‹ä¸€ä¸‹ VAE é‡å»ºçš„æŒ‡æ ‡

> Tab. 8 shows hyperparameters and reconstruction performance of the first stage models used for the LDMs com

![tb8](docs/2021_12_CVPR-highlight_High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models_Note/tb8.png)





å¯¹æ¯”ä¸åŒåŽ‹ç¼©æ¯”è®­ç»ƒçš„æ¨¡åž‹ï¼Œè®­ç»ƒæ—¶é•¿ & æ•ˆæžœ -> FID

LDM-1 è®­ç»ƒ 2M steps éƒ½è¿˜æ²¡æ”¶æ•›ã€‚ã€‚ã€‚**ç¡®å®žæœ€åŽçš„ f8-16 éƒ½è¿˜å¯ä»¥ï¼Œèƒ½æ­£å¸¸è®­å‡ºæ¥**ã€‚æŽ¨ç†ç”¨ 100steps DDIM

> Fig. 6 shows sample quality as a function of training progress for 2M steps of class-conditional models on the ImageNet [12] dataset. We see that, i) small downsampling factors for LDM-{1,2} result in slow training progress, whereas ii) overly large values of f cause stagnating fidelity after comparably few training steps.

> Results obtained with 100 DDIM steps [84] and Îº = 0.

![fig6](docs/2021_12_CVPR-highlight_High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models_Note/fig6.png)



- Qï¼šä¸ºå•¥ LDM-1&2 è®­å¾—æ…¢ï¼ŒLDM-32å¾ˆåžƒåœ¾ï¼Ÿ

> Revisiting the analysis above (Fig. 1 and 2) we attribute this to i) leaving most of perceptual compression to the diffusion model and ii) too strong first stage compression resulting in **information loss and thus limiting the achievable quality**

VAE åŽ‹ç¼©æ¯”å¤ªå¤§å¤ªæ‹‰äº†

![fig1](docs/2021_12_CVPR-highlight_High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models_Note/fig1.png)





å¯¹æ¯”ä¸åŒåŽ‹ç¼©æ¯”ï¼Œåœ¨ CelebA & ImageNet ä¸Šæ•ˆæžœï¼Œ**F8,16 ç¡®å®žå¯ä»¥**

![fig7](docs/2021_12_CVPR-highlight_High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models_Note/fig7.png)



æ€»ç»“ä¸€ä¸‹å°±æ˜¯ LDM-4-8 æ•ˆæžœæœ€å¥½

> In summary, LDM-4 and -8 offer the best conditions for achieving high-quality synthesis results.





### Image Generation with Latent Diffusion

å’Œ GAN æ–¹æ³•ï¼Œå¯¹æ¯”å›¾åƒç”Ÿæˆæ•ˆæžœ

> Moreover, LDMs consistently improve upon GAN-based methods in Precision and Recall, thus confirming the advantages of their mode-covering likelihood-based training objective over adversarial approaches.

- Qï¼šä¸ºå•¥è¯´æ¯” GAN å¥½ï¼Ÿ

è™½ç„¶å¯¹æ¯”å‡ ä¸ª FID å¼±äº†ä¸€äº›ï¼ˆå‡ ä¹ŽæŽ¥è¿‘ï¼‰ï¼Œä½† Precision Recall æ›´é«˜ï¼Œå¯¹äºŽæ•´ä½“æ•°æ®åˆ†å¸ƒå»ºæ¨¡æ›´å¥½

> Moreover, LDMs consistently improve upon GAN-based methods in **Precision and Recall,** thus confirming the advantages of their **mode-coverin**g likelihood-based training objective over adversarial approaches.

![tb1](docs/2021_12_CVPR-highlight_High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models_Note/tb1.png)

![tb2](docs/2021_12_CVPR-highlight_High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models_Note/tb2.png)



### Conditional

1.45B æ¨¡åž‹

>  For textto-image image modeling, we train a 1.45B parameter KL-regularized LDM conditioned on language prompts on LAION-400M [78]
>
> We employ the BERT-tokenizer [14] and implement Ï„Î¸ as a transformer [97] to infer a latent code which is mapped into the UNet via (multi-head) crossattention (Sec. 3.3). 



Layout-to-Image

![fig8](docs/2021_12_CVPR-highlight_High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models_Note/fig8.png)







256x256 è®­ç»ƒçš„æ¨¡åž‹ï¼Œå¯ä»¥ç›´æŽ¥æŽ¨ç† 512x1024! è¿›ä¸€æ­¥é™ä½Žè®­ç»ƒè¦æ±‚ :star:

![fig9](docs/2021_12_CVPR-highlight_High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models_Note/fig9.png)



SR

> - "Image super-resolution via iterative refinement"SR3

![fig10](docs/2021_12_CVPR-highlight_High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models_Note/fig10.png)



#### inpaint

Inpainting ä»»åŠ¡ä¸Šå¯¹æ¯”ä¸€ä¸‹ LDM vs DM æé€Ÿ x2.7, æŒ‡æ ‡ x1.6 :star:

> Tab. 6 reports the training and sampling throughput at resolution 2562 and 5122 , the total training time in hours per epoch and the FID score on the validation split after six epochs. Overall, we observe a speed-up of at least 2.7Ã— between pixel- and latent-based diffusion models while improving FID scores by a factor of at least 1.6Ã—.

![tb5-6](docs/2021_12_CVPR-highlight_High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models_Note/tb5-6.png)

çœ‹ä¸‹ inpaint æŒ‡æ ‡ -> SOTA

![tb7](docs/2021_12_CVPR-highlight_High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models_Note/tb7.png)

![fig11](docs/2021_12_CVPR-highlight_High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models_Note/fig11.png)





## Code



> pytorch-lighting  >> save moves for zero the gradient,...
> [stable-diffusion code analysis blog](https://zhuanlan.zhihu.com/p/613337342)
>
> - Summary
>
>   æœ‰äº† x å’Œ condition info ( class_name )
>
>   1. å°† x é€šè¿‡ VQGAN encoder æ˜ å°„ä¸º latent code
>
>   2. DDPM è®­ç»ƒ
>
>      1. ä»Ž 0-1000 éšæœºå– timestep 
>
>      2. condition  ç»è¿‡ `ClassEmbedder `æ˜ å°„å¾—åˆ° 1x512 tensor
>
>      3. `def p_losses` ä»Žå¹²å‡€ x0 åŠ å™ª T æ­¥
>
>         éšæœºå–å™ªå£° noise $\epsilon$ (shape å’Œ x0 ä¸€æ ·)
>
>         `def q_sample` æŒ‰å…¬å¼åŠ å™ª $q(x_t | x_0)\sim \mathcal{N}(\sqrt{\bar{a_t}} x_0, (1-\bar{a_t})I)$
>
>      4. è°ƒ U-net è¾“å…¥ x_t, t, condition é¢„æµ‹å™ªå£°ï¼Œä¸Žä¹‹å‰éšæœºå–çš„ noise $\epsilon$ è®¡ç®— L2 loss

```
# training config
configs/autoencoder/autoencoder_kl_64x64x3.yaml  # dataset image->256x256x3
configs/latent-diffusion/cin-ldm-vq-f8.yaml
```

- Loading module from yaml info >> use same code to initialize different classes

  ```python
  import importlib
  
  """ yaml example
      unet_config:
        target: ldm.modules.diffusionmodules.openaimodel.UNetModel
        params:
          image_size: 32
  """
  
  def instantiate_from_config(config):
      if not "target" in config:
          if config == '__is_first_stage__':
              return None
          elif config == "__is_unconditional__":
              return None
          raise KeyError("Expected key `target` to instantiate.")
      return get_obj_from_str(config["target"])(**config.get("params", dict()))
  
  
  def get_obj_from_str(string, reload=False):
      module, cls = string.rsplit(".", 1)
      if reload:
          module_imp = importlib.import_module(module)
          importlib.reload(module_imp)
      return getattr(importlib.import_module(module, package=None), cls)
  ```

  

### AutoencoderKL

`def training_step` main training procedures happens

> VQGAN code learning

- **LPIPS Loss >> perceptual loss**

  input & reconstruction from Autoencoder all shapes like [B, 3, 256, 256]

  use **pretrained `VGG16`'s feature Module**, which is `nn.Sequential` hence the layer could be visited by index. What `LPIPS` do is **split the starting 30 layers into 5 slice `nn.Sequential`** (cut at layer indexing by 3,8,15,22,29 and **each module ends with layer `RELU(inplace=True)`**)

  ```python
  from torchvision import models
  
  vgg_pretrained_features = models.vgg16(pretrained=pretrained).features  # nn.Sequential 31 layers
  # [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M']
  # number-> ConvModule(conv2d+relu) "M"->Maxpooling
  
  # output with 5 moudle's output (each end with RELU layer)
  vgg_outputs = namedtuple("VggOutputs", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3', 'relu5_3'])
  ```

  > What could we learn from this LPIPS implementationï¼Ÿ
  >
  > **tear pretrained Module apart to get intermediate layers output** 
  >
  > ```python
  > class vgg16(torch.nn.Module):
  >  def __init__(self, requires_grad=False, pretrained=True):
  >      super(vgg16, self).__init__()
  >      vgg_pretrained_features = models.vgg16(pretrained=pretrained).features  # nn.Sequential hence could be visit by index 
  >      self.slice1 = torch.nn.Sequential()
  >      for x in range(4):
  >          self.slice1.add_module(str(x), vgg_pretrained_features[x]) 
  > 
  >      # ...
  > 	def forward(self, X): 
  >      h = self.slice1(X)
  >      h_relu1_2 = h
  >      # ...
  >      vgg_outputs = namedtuple("VggOutputs", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3', 'relu5_3'])
  >      return vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3, h_relu5_3)
  > 
  > ```

  

- download file standard ways

  ```python
  def download(url, local_path, chunk_size=1024):
      os.makedirs(os.path.split(local_path)[0], exist_ok=True)
      with requests.get(url, stream=True) as r:
          total_size = int(r.headers.get("content-length", 0))
          with tqdm(total=total_size, unit="B", unit_scale=True) as pbar:
              with open(local_path, "wb") as f:
                  for data in r.iter_content(chunk_size=chunk_size):
                      if data:
                          f.write(data)
                          pbar.update(chunk_size)
  ```

- import module from string in config file

  ```python
  import importlib
  
  def get_obj_from_str(string, reload=False):
      module, cls = string.rsplit(".", 1)
      if reload:
          module_imp = importlib.import_module(module)
          importlib.reload(module_imp)
      return getattr(importlib.import_module(module, package=None), cls)
  ```

  

- KL loss

  > [How to generate Gaussian samples](https://medium.com/mti-technology/how-to-generate-gaussian-samples-347c391b7959)
  >
  > - Variance æ–¹å·®
  > - Standard Deviation
  >   Standard deviation is a statistic that measures the dispersion of a dataset relative to its [mean](https://www.investopedia.com/terms/m/mean.asp) and is calculated as the square root of the [variance](https://www.investopedia.com/terms/v/variance.asp). 
  > - KL divergence [blog](https://zhuanlan.zhihu.com/p/438129018)

  `class DiagonalGaussianDistribution` >> `ldm/modules/distributions/distributions.py`

  - sample

    `logvar` >> $\log{(\sigma^2)}$
    $$
    \text{assume } z\sim\mathcal{N}(0,1) \\
    if \\
    x = \mu + \sigma * Z = \mu + exp(0.5 * \log{(\sigma^2)}) * Z \\
    \text{lets say } x\sim\mathcal{N}(\mu, \sigma)\\
    $$

  - KL divergence
    $$
    KL\_Loss = 0.5 *\sum{(\mu^2 + \sigma^2 - 1 - \log{\sigma^2})}
    $$

  - VQ-GAN adaptive weight
    $$
    Loss = \mathcal{L}_{resc} +\lambda * \mathcal{L}_{GAN} \\
    \lambda = \frac{\nabla_{GL}{\mathcal{L}_{resc}}}{\nabla_{GL}{\mathcal{L}_{GAN}} +1e-4}
    $$
    **èŽ·å–æœ€åŽä¸€å±‚ layer çš„æ¢¯åº¦**

    ```
    last_layer = self.decoder.last_layer.weight
    
    nll_grads = torch.autograd.grad(nll_loss, last_layer, retain_graph=True)[0]
    ```

- PatchGAN discriminator

  > [Question: PatchGAN Discriminator](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/39)
  >
  > In fact, a "PatchGAN" is just a convnet! Or you could say all convnets are patchnets: the power of convnets is that they process each image patch identically and independently. å·ç§¯æ ¸å¤„ç†çš„æ—¶å€™å°±æ˜¯ä¸€ä¸ª image patch

  

### LatentDiffusion Train

>  in `ldm/models/diffusion/ddpm.py`
>  configure file=`configs/latent-diffusion/cin-ldm-vq-f8.yaml`

initialize the `class DDPM(pl.LightningModule)`

#### UNetModel

> U-net model in `self.model = DiffusionWrapper(unet_config, conditioning_key)`
> `class UNetModel(nn.Module):`  >> `ldm/modules/diffusionmodules/openaimodel.py`
>
> *The full UNet model with attention and timestep embedding.* 
> See U-net structure >> [Stable-diffusion_U-net-structure-with-Note.pdf](Stable-diffusion_U-net-structure-with-Note.pdf)
>
> - [What is the 'zero_module' used for?](https://github.com/openai/guided-diffusion/issues/21)
>
>   is used to initialize certain modules to zero. 
>
>   ```python
>   def zero_module(module):
>       """
>       Zero out the parameters of a module and return it. >> Initialize the module with 0
>       """
>       for p in module.parameters():
>           p.detach().zero_()
>       return module
>   ```

![U-net structure](docs/2021_12_CVPR-highlight_High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models_Note/stable_diffusion_unet_architecture.png)

> Upsample ä¸€èˆ¬éƒ½æ˜¯å…ˆ Upsample å†æŽ¥ `Conv2d(kernel_size=3,stride=1,padding=1)`





#### **timestep_embedding**

> `ldm/modules/diffusionmodules/util.py`
> [timestep embedding blog](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/#what-is-positional-encoding-and-why-do-we-need-it-in-the-first-place) :star:
> [detailed proof for]

**basics**

the time-embedding should satisfy the following criteria:

1. unique each  timestep(word's position)
2. distance between 2 time-step should be consistent across different sentence
3. generalize to longer sentences
4. determinstic

using binary values would be a waste of space in the world of floats. So instead, we can use their float continous counterparts - **Sinusoidal functions. Indeed, they are the equivalent to alternating bits.** 

<figure>
    <img src="https://d33wubrfki0l68.cloudfront.net/ef81ee3018af6ab6f23769031f8961afcdd67c68/3358f/img/transformer_architecture_positional_encoding/positional_encoding.png">
    <figcaption> 
        <center>
        <I>Sinusoidal embedding for dim=50
        </center>
    </figcaption>
</figure>




***sinusoidal timestep embeddings.***

$POS_{i} * POS_{i+k}$ éšç€é—´éš” K å¢žå¤§ï¼Œé€æ¸å‡å°æ¥è¡¨ç¤ºç›¸å¯¹ä½ç½®å…³ç³»ã€‚ä½†å†…ç§¯æ˜¯å¯¹ç§°çš„ï¼Œæ— æ³•åŒºåˆ†å‰åŽå…³ç³»

```python
freqs = torch.exp(-math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half).to(device=timesteps.device)
args = timesteps[:, None].float() * freqs[None]  # [N,1] * [1, half] >> [N, half]
embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)  # [N, dim]

time_embed_dim = model_channels * 4
time_embed = nn.Sequential(
            linear(model_channels, time_embed_dim),
            nn.SiLU(),
            linear(time_embed_dim, time_embed_dim),
        )
```



**LitEma**

æ»‘åŠ¨å¹³å‡



**register_schedule**

> Whole bunch of formulations could be check at [DDPM video 42:19](https://www.youtube.com/watch?v=y7J6sSO1k50)

- beta >> linear 
  `torch.linspace(linear_start ** 0.5, linear_end ** 0.5, n_timestep, dtype=torch.float64) ** 2`

- alpha $[1, 0.9999, \cdots, 7e-4]$

- $\bar{\alpha_i}$ 

  ```
  alphas_cumprod = np.cumprod(alphas, axis=0)
  alphas_cumprod_prev = np.append(1., alphas_cumprod[:-1])
  ```



### **VQModelInterface**

> `ldm.models.autoencoder.VQModelInterface`



### LitEMA :open_hands:

> [blog: stable-diffusion optimization: EMA weights on CPU](https://lunnova.dev/articles/stable-diffusion-ema-on-cpu/)
> Stable Diffusion çš„ LitEMA æ»‘åŠ¨å¹³å‡å›žåˆ°æ˜¯å‚æ•°åœ¨ GPU æŸä¸€æ—¶åˆ»åŠ å€ï¼Œå¯èƒ½å¯¼è‡´æ˜¾å­˜æº¢å‡ºï¼å¯ä»¥æ ¹æ®æ­¤ blog ä¿®æ”¹ `LitEMA()` å°†å‚æ•°å­˜åœ¨ CPU ä¸Šï¼Œæ®è¯´å¯ä»¥ä»Ž 32G -> 27G æ˜¾å­˜

Stable diffusion uses an Exponential Moving Average of the model's weights to improve quality of resulting images and **avoid overfitting to the most recently trained images.**

Stable Diffusion includes an implementation of an EMA called `LitEma`, found at [ldm/modules/ema.py](https://github.com/CompVis/stable-diffusion/blob/69ae4b35e0a0f6ee1af8bb9a5d0016ccb27e36dc/ldm/modules/ema.py)

- How do you implement an EMA for a machine learning model?

  With PyTorch modules you can use [the `named_parameters()` iterator](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_parameters) to access all parameters. :star:

- the EMA weights end up on the GPU like everything else. This **doubles the memory required** to store the model parameters! å¯èƒ½é€ æˆæ˜¾å­˜æº¢å‡º !! :warning:

  > [How to keep some LightningModule's parameters on cpu when using CUDA devices for training](https://github.com/Lightning-AI/lightning/issues/3698)



### Text2Image

> `scripts/txt2img.py`

#### PLMS

> [PNDM github repo](https://github.com/luping-liu/PNDM)
> [arXiv: Pseudo Numerical Methods for Diffusion Models on Manifolds](https://arxiv.org/abs/2202.09778)
> [local pdf](./2022_ICLR_PNDM_Pseudo-Numerical-Methods-for-Diffusion-Models-on-Manifolds.pdf)

- [invisible-watermark](https://github.com/ShieldMnt/invisible-watermark)



### finetune SDv2-inpaint

> [diffuser baisc training](https://huggingface.co/docs/diffusers/tutorials/basic_training)
>
> SDv2-inpaint checkpoint https://huggingface.co/stabilityai/stable-diffusion-2-inpainting :star:

This `stable-diffusion-2-inpainting` model is resumed from [stable-diffusion-2-base](https://huggingface.co/stabilityai/stable-diffusion-2-base) (`512-base-ema.ckpt`) and trained for another 200k steps.

- Qï¼šè¾“å…¥ï¼Ÿ

```python
z, c, x, xrec, xc = super().get_input(batch, self.first_stage_key, return_first_stage_outputs=True,force_c_encode=True, return_original_cond=True, bs=bs)
# inpaint: [z: jpg_vae_fea, c:text_fea, x:jpg, xrec: vae_Recon, xc: text]
bchw = z.shape

c_cat = list()  # 
# mask, resize to z's shape
cc = torch.nn.functional.interpolate(cc, size=bchw[-2:])
c_cat.append(cc)
# masked_image, apply vae-encde; 
cc = self.get_first_stage_encoding(self.encode_first_stage(cc))
c_cat.append(cc)

c_cat = torch.cat(c_cat, dim=1)
all_conds = {"c_concat": [c_cat], "c_crossattn": [c]}
```

åˆå§‹å™ªå£° xTï¼Œè®­ç»ƒæ—¶å€™ç›´æŽ¥å¯¹ GT åŠ å™ª

```
prng = np.random.RandomState(seed)
start_code = prng.randn(num_samples, 4, h // 8, w // 8)
start_code = torch.from_numpy(start_code).to(
device=device, dtype=torch.float32)
```

- SDv2-inpaint UNet è¾“å…¥ [code](https://vscode.dev/github/Stability-AI/stablediffusion/blob/main/ldm/models/diffusion/ddpm.py#L1346)

å°† noise_fea(c=4), mask(c=1), masked_image_fea(c=4) åˆå¹¶èµ·æ¥è¾“å…¥ Unetï¼Œ**è°ƒæ•´ unet çš„ conv_in çš„ C=9 é€šé“**

```
        elif self.conditioning_key == 'hybrid':
            xc = torch.cat([x] + c_concat, dim=1)
            cc = torch.cat(c_crossattn, 1)
            out = self.diffusion_model(xc, t, context=cc)
```

Framework å‚è€ƒ BrushNet çš„ Controlnet éƒ¨åˆ†çš„è¾“å…¥

![ComparsionDiffusionFramework.png](docs/2021_12_CVPR-highlight_High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models_Note/ComparsionDiffusionFramework.png)





- Qï¼šSDv2 github repo ä»£ç åŸºäºŽ pytorch lightningï¼Œä¸æ–¹ä¾¿

é‡å†™ä¸€ä¸ª diffuser çš„ï¼Œæ‰¾ä¸€ä¸ª diffusers SDv2 çš„ä»£ç  :star:

> issue https://github.com/huggingface/diffusers/issues/1392#issuecomment-1326349638 >> https://huggingface.co/stabilityai/stable-diffusion-2-inpainting

```python
from diffusers import StableDiffusionInpaintPipeline
pipe = StableDiffusionInpaintPipeline.from_pretrained(
    "stabilityai/stable-diffusion-2-inpainting",
    torch_dtype=torch.float16,
)
pipe.to("cuda")
prompt = "Face of a yellow cat, high resolution, sitting on a park bench"
#image and mask_image should be PIL images.
#The mask structure is white for inpainting and black for keeping as is
image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]
image.save("./yellow_cat_on_park_bench.png")
```



- Qï¼šæ²¡æœ‰ controlnet or Lora å¦‚ä½• finetuneï¼Ÿ

> SDv2 inpaint finetune model [code](https://github.com/Stability-AI/stablediffusion/blob/cf1d67a6fd5ea1aa600c4df58e5b47da45f6bdbf/ldm/models/diffusion/ddpm.py#L1504)

åªå¾®è°ƒä¸€ä¸ª input block

```python
                 finetune_keys=("model.diffusion_model.input_blocks.0.0.weight",
                                "model_ema.diffusion_modelinput_blocks00weight"
                                ),
```





## Limitations

- æŽ¨ç†æ¯” GAN æ…¢å¤šäº†ã€‚ã€‚ã€‚

> While LDMs significantly reduce computational requirements compared to pixel-based approaches, their sequential sampling process is still slower than that of GANs.

- VAE é‡å»ºè¿˜æ˜¯æœ‰æŸå¤± -> é«˜ç»†èŠ‚è¦æ±‚çš„åœºæ™¯å¾ˆéš¾ç”¨

> Moreover, the use of LDMs can be questionable when high precision is required: although the loss of image quality is very small in our f = 4 autoencoding models (see Fig. 1), their reconstruction capability can become a bottleneck for tasks that require fine-grained accuracy in pixel space. 



![fig1](docs/2021_12_CVPR-highlight_High-Resolution-Image-Synthesis-with-Latent-Diffusion-Models_Note/fig1.png)



## Summary :star2:

> learn what

### how to apply to our task

