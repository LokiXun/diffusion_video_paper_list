# LVCD: Reference-based Lineart Video Colorization with Diffusion Models

> "LVCD: Reference-based Lineart Video Colorization with Diffusion Models" SIGGRAPH, 2024 Sep 19
> [paper](http://arxiv.org/abs/2409.12960v1) [code](https://github.com/luckyhzt/LVCD) [pdf](./2024_09_SIGGRAPH_LVCD--Reference-based-Lineart-Video-Colorization-with-Diffusion-Models.pdf) [note](./2024_09_SIGGRAPH_LVCD--Reference-based-Lineart-Video-Colorization-with-Diffusion-Models_Note.md)
> Authors: Zhitong Huang, Mohan Zhang, Jing Liao

## Key-point

- Task: reference-based lineart video colorization

  > ç»™å®šä¸€ä¸ª sketch åºåˆ—ï¼Œä¸€å¼ ä¸Šè‰²å¥½çš„å‚è€ƒå›¾ï¼Œç”Ÿæˆ colorized video

- Problems

  - high-quality, long temporal-consistent animation videos

- :label: Label:

## Contributions

- We propose the **first video diffusion framework for reference-based lineart animation colorization**, harnessing the capabilities of a pretrained video diffusion model to generate long, temporally consistent animations of high quality
- ä½¿ç”¨ Sketch ControlNet ä¿æŒç»“æ„ & å¸ƒå±€ï¼›è®¾è®¡ä¸€ä¸ª reference attention ä»£æ›¿ SVD åŸå§‹ self-attention

> To achieve this, we first extend the ControlNet [Zhang et al. 2023] to a video version, termed Sketch-guided ControlNet, incorporating additional lineart sketch control as a crucial guide for the animationâ€™s layout and structure.
>
> we introduce **Reference Attention to replace the original spatial attention** layers in SVD, facilitating long-range spatial matching between the first reference frame and consecutive generated frames

- SVD é¢„è®­ç»ƒæ¨¡å‹ä¸€æ¬¡åªèƒ½å‡º 14 å¸§ï¼Œå› æ­¤è®¾è®¡äº†ä¸€ä¸‹æ¨ç†ç­–ç•¥

> we introduce a novel scheme for sequential sampling, including **Overlapped Blending Module and Prev-Reference Attention**, to enable the colorization of long animation.





## Introduction

- Qï¼šåŠ¨æ¼«è§†é¢‘ä¸Šè‰²è€—æ—¶ã€‚ã€‚

> However, the production of animation remains a labor-intensive and time-consuming endeavor. Traditionally, artists manually sketch and colorize keyframes, leaving the in-between frames to be completed in accordance with the style and color palette of the keyframes
>
> This filling process, while essential for preserving consistency and coherence in the animation, can be highly repetitive and tedious for the artists involved.

### lineart video colorization

- ä¹‹å‰å·¥ä½œé€å¸§ä¸Šè‰²ï¼Œåªè€ƒè™‘ç›¸é‚»å¸§ä¹‹é—´çš„ä¸€è‡´æ€§ï¼Œ**è·ç¦»å·®è· 10 å¸§ä¸€è‡´æ€§è¾ƒå·®**

> While previous works have attempted lineart video colorization, their frameworks typically rely on image generative models that process one frame at a time, focusing solely on the coherence between neighboring frames. 

ä½¿ç”¨å‰é¢ç”Ÿæˆå¥½çš„å¸§ä½œä¸º referenceï¼Œå­˜åœ¨è¶…çº§ä¸¥é‡çš„è¯¯å·®ç´¯ç§¯ï¼Œçœ‹ Fig4 EISAIï¼ŒSEINE æ–¹æ³•ï¼Œæ‹“å±•åˆ° 30 å¸§å°±æ²¡æ³•çœ‹äº†ï¼ï¼

> To maintain temporal consistency in the generated video, most of these works employ the previously generated frame as the new reference frame to produce subsequent frames. Unfortunately, this propagation can lead to **significant error accumulation and artifacts,** even within 10 consecutive samplings (an example is provided in 2 ğ‘›ğ‘‘ and 3 ğ‘Ÿğ‘‘ rows of Fig. 4)

![fig4](docs/2024_09_SIGGRAPH_LVCD--Reference-based-Lineart-Video-Colorization-with-Diffusion-Models_Note/fig4.png)



- åŸºäº GAN çš„æ–¹æ³•ç”Ÿæˆèƒ½åŠ›æ¯” diffusion å¼±ï¼Œæœ‰ä¼ªå½±

> Another drawback of previous approaches is they all use frameworks based on Generative Adversarial Networks [Goodfellow et al. 2014] (GANs), which have limited generative ability compared to more recent architectures such as transformers [Vaswani et al. 2017] and diffusion models [Song et al. 2021]



### SVD as base

ä½¿ç”¨ SVD ä½œä¸ºåŸºç¡€æ¨¡å‹

> Following extensive evaluation, we select Stable Video Diffusion [Blattmann et al. 2023] (SVD) as the base model for our framework.

1.  temporal video model facilitates the **generation of temporally consistent videos**, surpassing image generative models
2. the large amount of training data empowers SVD with robust generative capabilities to synthesize high-fidelity videos



SVD åº”ç”¨åˆ°è¿™ä¸ªä¸Šè‰²ä»»åŠ¡æœ‰æŒ‘æˆ˜

> While SVD provides a robust foundation for our proposed task, we must address three significant challenges to adapt it accordingly

1. éœ€è¦æ˜ å…¥ sketches frames ä½œä¸º conditionï¼ŒåŸå§‹ SVD æ˜¯ä¸ª image2video æ¨¡å‹

   > only supports the conditioning from a reference image

2. åŠ¨ç”»ä¸­ç»†å¾®çš„åŠ¨ä½œç”Ÿæˆï¼ŒSVD ç”Ÿæˆçš„åŠ¨ä½œè¿˜æ˜¯æœ‰äº›ä¸è‡ªç„¶

   > Adaptation to expansive motions: While SVD is limited to produce videos with subtle motions, animation clips often feature larger motions. Thus, modifications to the original SVD are necessary to accommodate expansive motions in animations

3. è¶…é•¿è§†é¢‘ï¼ŒåŸå§‹ SVD åªæ”¯æŒ14/25 å¸§

   > Extension to long video: The original SVD is restricted to generating videos of fixed length, which does not meet the requirements for animation colorization



ä¹‹åä»‹ç» controbution



### Related

åˆ†ä¸ºä¸€ä¸‹å†…å®¹ä»‹ç»ï¼šåŠ¨ç”»ä¸Šè‰²ï¼ŒåŸºäºå‚è€ƒå›¾çš„ä¸Šè‰²ï¼Œè§†é¢‘æ’å¸§ï¼Œvideo diffusion



- "SEINE: Short-to-Long Video Diffusion Model for Generative Transition and Prediction" ICLR
- AnimeDiffusion æ”¯æŒç›´æ¥ä» sketch åˆ° colorized videoï¼Œä½† temporal ä¸€è‡´æ€§å¤ªçƒ‚
  - https://github.com/xq-meng/AnimeDiffusion

éœ€è¦åšæˆ reference-based çš„å½¢å¼

> The first diffusion-based framework for anime face lineart colorization, based on reference images, was proposed in [Cao et al. 2024]. Despite the advancements in lineart image colorization, these frameworks have limitations in producing videos without accounting for temporal coherence between colorized frames.







## methods

![fig2](docs/2024_09_SIGGRAPH_LVCD--Reference-based-Lineart-Video-Colorization-with-Diffusion-Models_Note/fig2.png)

- ç›¸æ¯” SD å¤šäº† Temporal CrossAttention



### Reference Attention

åˆ†æäº† SVD æ—¶åºä¸æ”¯æŒå¤§èŒƒå›´åŠ¨ä½œçš„åŸå› 

- ZT å’Œ noise åœ¨ C ä¸Š concat é™åˆ¶äº†é•¿èŒƒå›´ alignment
- temporal attention åªåœ¨ 1D ä¸Šåšï¼Œæ²¡å¯¹ä¸åŒä½ç½®åš attn

> Firstly, the encoded reference latents are concatenated with the noised latents along the channel dimension to form the video input [ğ‘¥ 0 , ğ‘¥ğ‘– ğ‘‡ ],ğ‘– = 1, ..., ğ‘, as shown in Fig. 2. This spatial alignment of the two inputs prevents effective long-range matching between them.

### Overlapped Blending Module.

æ‰€æœ‰timestep çš„ zt æ¢ä¸ºä¹‹å‰é‡å éƒ¨åˆ†çš„

![fig3](docs/2024_09_SIGGRAPH_LVCD--Reference-based-Lineart-Video-Colorization-with-Diffusion-Models_Note/fig3.png)



## setting

## Experiment

> ablation study çœ‹é‚£ä¸ªæ¨¡å—æœ‰æ•ˆï¼Œæ€»ç»“ä¸€ä¸‹

## Limitations

## Summary :star2:

> learn what

### how to apply to our task

- Qï¼šreference image å¦‚ä½•ä½¿ç”¨ï¼Ÿ
- Qï¼šæ—¶åºä¸€è‡´æ€§ï¼Ÿ
- Qï¼šæŒ‡æ ‡



