# EMP-SSL: Towards Self-Supervised Learning in One Training Epoch

> "EMP-SSL: Towards Self-Supervised Learning in One Training Epoch" Arxiv, 2023 Apr, **EMP-SSL**
> [paper](https://arxiv.org/abs/2304.03977) [code](https://github.com/tsb0601/EMP-SSL) [blog_explanation](https://mp.weixin.qq.com/s/OJphdhUrihKSVj14b6gLmA)
> [local pdf](./2023_04_Arxiv_EMP-SSL--Towards-Self-Supervised-Learning-in-One-Training-Epoch.pdf)

## **Key-point**

ä¸»è¦è§£å†³**è‡ªç›‘ç£å­¦ä¹ **è®­ç»ƒæ•ˆç‡ä½çš„é—®é¢˜ã€‚è®¾è®¡äº†ä¸€ä¸ª loss ä½¿å¾—ç›¸ä¼¼çš„å›¾ï¼Œå¾—åˆ°çš„  latent representation æ›´åŠ æ¥è¿‘ï¼Œå¹¶ä¸”è¿™æ ·**å­¦ä¹ åˆ°çš„ latent representation æœ‰æ„ä¹‰ & è®­ç»ƒåŠ é€Ÿ& å¾ˆå¥½çš„è¿ç§»æ€§ && æœ‰åŒºåˆ†åº¦** >> æ€è€ƒæ€ä¹ˆç”¨äº diffusion åŠ é€Ÿ ï¼:star:

å‘ç° Joint-embedding æ–¹æ³•æé«˜æ•ˆç‡çš„å…³é”®æ˜¯å¢åŠ  patches çš„æ•°é‡ï¼ˆ1 epoch è®­åˆ°å¾ˆå¥½çš„æ•ˆæœï¼‰ï¼Œ1 ä¸ª epoch å°±åœ¨ CIFAR10 ä¸Šæ”¶æ•›åˆ° 85%



**Contributions**

- å®éªŒå‘ç°ä¸€ç§è‡ªç›‘ç£å­¦ä¹ æ–¹å¼ï¼Œ**å­¦åˆ°çš„ latent code æ›´æœ‰æ„ä¹‰ï¼ŒåŠ é€Ÿæ”¶æ•›**



## **Related Work**

- Linear probe

  linear probing â€“ a collision resolution technique for searching the location of an element in a hash table.

- **learning more discriminative latent code like generative models** :star:
  "Training GANs with Stronger Augmentations via Contrastive Discriminator" Arxiv, 2021 Mar
  [paper](https://arxiv.org/abs/2103.09742)

- SSL(Self-Supervised Learning)

- LARS ä¼˜åŒ–å™¨

  > [blog](https://www.jianshu.com/p/e430620d3acf) :+1:

  ç”¨äºå¤§ batchsize æ—¶è¿›è¡Œä¼˜åŒ–ï¼Œå¯¹æ¨¡å‹æ¯ä¸ª layer å•ç‹¬è®¾ç½® learning rateï¼Œé¿å… batchsize å¢å¤§ learning rate è®¾ç½®å¯¼è‡´çš„ä¸ç¨³å®šé—®é¢˜ã€‚



## **methods**

**ä¸»è¦å…³æ³¨è®¾è®¡çš„ loss ä¼˜åŒ– latent representation**ï¼Œå›¾åƒçš„ Encoder å’Œ Injection Mode ä¸º Res18 å’Œ FC

> EMP-SSL æ²¡æœ‰æ¢ç´¢ prediction networks, momentum encoders, non-differentiable operators, or stop gradients è¿›ä¸€æ­¥æå‡æ€§èƒ½çš„å½±å“ >> future work



### Loss with Total Coding Rate(TCR) :star2: 

ä¸€ç§åæ–¹å·®æ­£åˆ™åŒ–æ–¹æ³•

- Total Coding Rate (TCR) [36, 35, 53, 15], which is a covariance regularization technique, to **avoid collapsed representation**
  $$
  R(Z) = \frac{1}{2} \log{\det{(I + \frac{feature\_dim}{batch\_num * \epsilon^2}Z*Z^T)}}
  $$
  

**covariance regularization is achieved by maximizing the Total Coding Rate (TCR).**

> è§†é¢‘ä¸­ï¼Œ**æœ‰å™ªå£°çš„ç›¸é‚»å¸§è®¤ä¸ºæ˜¯åŒä¸€ä¸ªåœºæ™¯çš„ augmentation** éœ€è¦ latent representation æ¥è¿‘

$$
Loss = \max{\frac{1}{n}\sum_{i=1}^{n}{(R(Z_i) ~+~\lambda\cdot D(Z_i, \bar{Z}))}}\\
\bar{Z} = \frac{1}{n}\sum_{i=1}^{n}{Z_i}\\
\text{where $n$ is augmented results number, $\bar{Z}$ is the mean of representations of different augmented
patches ,}\\
\text{In the TCR loss, Î» is set to 200.0 and $\epsilon^2$is set to 0.2 (Exp setting)}
$$

- $D(Z_i, \bar{Z_i})$ ä¸ºä½™å¼¦ç›¸ä¼¼åº¦å‡½æ•°



## **Experiment**

> ablation study çœ‹é‚£ä¸ªæ¨¡å—æœ‰æ•ˆï¼Œæ€»ç»“ä¸€ä¸‹

We use a ResNet-18 [27] as the backbone and train for at most 30 epochs. We use a batch size of 100, the LARS optimizer [51] with Î· set to 0.005, and a weight decay of 1e-4.  In the TCR loss, Î» is set to 200.0 and  2 is set to 0.2

### CIFAR-100 Cls

Accuracy is measured by **training linear classifier on learned embedding representation**

åˆ†ç±»å‡†ç¡®ç‡æŒ‡æ ‡ & æ”¶æ•›é€Ÿåº¦å‡æœ€ä¼˜ï¼Œå¾ˆéœ‡æ’¼ :zap:

<table><tr>
    <td><img src='./docs/EMP-SSL_CIFAR_quantitative_result.png'></td>
    <td><img src='./docs/EMP-SSL_CIFAR_convergence_plot.png'></td>
</table>



Several Fining:

1. EMP-SSL 10 ä¸ª Epoch æ•ˆæœï¼Œä¼˜äºå…¶ä»– SOTA SSL æ–¹æ³• 1000 ä¸ª epochçš„ç»“æœ
2. patches æ•°é‡å¢åŠ   $20 \rightarrow 200$ æ•ˆæœæœ‰æ›´ä¼˜



### ablation study

- **number of patches $n$** to illustrate the importance of patch number in joint-embedding SSL

  patches =100 å°±å¾ˆå¥½äº†

- Transferability ä½†éƒ½è¿˜æ˜¯åˆ†ç±»ä»»åŠ¡åªæ˜¯æ•°æ®é›†ä¸ä¸€æ ·

  > Note that despite similar names, CIFAR-10 and CIFAR-100 have very little overlap hence they are suitable for testing modelâ€™s transferability.

  the methodâ€™s better transferability to out of domain data

  (1) models pretrained on CIFAR-10 and linearly evaluated on CIFAR-100 (2) models pretrained on CIFAR-100 and linearly evaluated on CIFAR-10

  1. SOTA SSL æ–¹æ³•åœ¨ä¸€ä¸ªä¸Šé¢å¥½æ¢æˆå¦ä¸€ä¸ªå°±ä¸è¡Œï¼Œ**è¯´æ˜ SSL çš„ latent representation éœ€è¦ generalize well to out-of-domain data instead of overfitting the training data**
  2. a larger number of training epochs causes the models to overfit to the training dataset.

  

$*v = \frac{v}{\max(\lVert v \rVert_p, \epsilon)}.*$

## Code

- res18

```python
from torchvision.models import resnet18, resnet34, resnet50

def getmodel(arch):
    """get resnet 18 model"""
    #backbone = resnet18()
    
    if arch == "resnet18-cifar":
        backbone = resnet18()
        backbone.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False) 
        backbone.maxpool = nn.Identity()
        backbone.fc = nn.Identity()
        return backbone, 512  
    elif arch == "resnet18-imagenet":
        backbone = resnet18()    
        backbone.fc = nn.Identity()
        return backbone, 512
    elif arch == "resnet18-tinyimagenet":
        backbone = resnet18()    
        backbone.avgpool = nn.AdaptiveAvgPool2d(1)
        backbone.fc = nn.Identity()
        return backbone, 512
    else:
        raise NameError("{} not found in network architecture".format(arch))
```

- Total Coding Rate
  $$
  R(Z) = \frac{1}{2} \log{\det{(I + \frac{feature\_dim}{batch\_num * \epsilon^2}Z*Z^T)}}
  $$
  

  - `torch.logdet` [doc](https://pytorch.org/docs/stable/generated/torch.logdet.html?highlight=logdet#torch.logdet)
    Calculates log determinant of a square matrix or batches of square matrice

  ```python
  class TotalCodingRate(nn.Module):
      def __init__(self, eps=0.01):
          super(TotalCodingRate, self).__init__()
          self.eps = eps
          
      def compute_discrimn_loss(self, W):
          """Discriminative Loss."""
          p, m = W.shape  #[d, B]
          I = torch.eye(p,device=W.device)
          scalar = p / (m * self.eps)
          logdet = torch.logdet(I + scalar * W.matmul(W.T))
          return logdet / 2.
      
      def forward(self,X):
          return - self.compute_discrimn_loss(X.T)
  ```

  



## **Summary :star2:**

> learn what & how to apply to our task

- **learning more discriminative latent code like generative models** :star:

  æ€è€ƒæ€ä¹ˆæ”¹è¿›ç”¨åˆ° diffusion é‡Œé¢ï¼Œå‚è€ƒ [related works å†™çš„é‚£ä¸ªæ–‡ç« ](#Related Work)

  å®ç° diffusion åŠ é€Ÿ

