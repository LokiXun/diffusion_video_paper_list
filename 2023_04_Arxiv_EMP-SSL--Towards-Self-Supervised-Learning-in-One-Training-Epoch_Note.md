# EMP-SSL: Towards Self-Supervised Learning in One Training Epoch

> "EMP-SSL: Towards Self-Supervised Learning in One Training Epoch" Arxiv, 2023 Apr, **EMP-SSL**
> [paper](https://arxiv.org/abs/2304.03977) [code](https://github.com/tsb0601/EMP-SSL) [blog_explanation](https://mp.weixin.qq.com/s/OJphdhUrihKSVj14b6gLmA)
> [local pdf](./2023_04_Arxiv_EMP-SSL--Towards-Self-Supervised-Learning-in-One-Training-Epoch.pdf)

## **Key-point**

ä¸»è¦è§£å†³è‡ªç›‘ç£å­¦ä¹ è®­ç»ƒæ•ˆç‡ä½çš„é—®é¢˜ï¼Œå‘ç° Joint-embedding æ–¹æ³•æé«˜æ•ˆç‡çš„å…³é”®æ˜¯å¢åŠ  patches çš„æ•°é‡ï¼ˆ1 epoch è®­åˆ°å¾ˆå¥½çš„æ•ˆæœï¼‰

è¿™æ ·å­¦ä¹ åˆ°çš„ latent representation æœ‰æ„ä¹‰ && å¾ˆå¥½çš„è¿ç§»æ€§ && æœ‰åŒºåˆ†åº¦ >> æ€è€ƒæ€ä¹ˆç”¨äº diffusion åŠ é€Ÿ ï¼:star:



**Contributions**

## **Related Work**

- Linear probe

  linear probing â€“ a collision resolution technique for searching the location of an element in a hash table.

- **learning more discriminative latent code like generative models** :star:
  "Training GANs with Stronger Augmentations via Contrastive Discriminator" Arxiv, 2021 Mar
  [paper](https://arxiv.org/abs/2103.09742)

- SSL(Self-Supervised Learning)



## **methods**

**ä¸»è¦å…³æ³¨è®¾è®¡çš„ loss ä¼˜åŒ– latent representation**ï¼Œå›¾åƒçš„ Encoder å’Œ Injection Mode ä¸º Res18 å’Œ FC

> EMP-SSL æ²¡æœ‰æ¢ç´¢ prediction networks, momentum encoders, non-differentiable operators, or stop gradients è¿›ä¸€æ­¥æå‡æ€§èƒ½çš„å½±å“ >> future work



### Loss with Total Coding Rate(TCR) :star2: 

ä¸€ç§åæ–¹å·®æ­£åˆ™åŒ–æ–¹æ³•

Total Coding Rate (TCR) [36, 35, 53, 15], which is a covariance regularization technique, to **avoid collapsed representation**

**covariance regularization is achieved by maximizing the Total Coding Rate (TCR).**

> è§†é¢‘ä¸­ï¼Œ**æœ‰å™ªå£°çš„ç›¸é‚»å¸§è®¤ä¸ºæ˜¯åŒä¸€ä¸ªåœºæ™¯çš„ augmentation** éœ€è¦ latent representation æ¥è¿‘

$$
Loss = \max{\frac{1}{n}\sum_{i=1}^{n}{(R(Z_i) ~+~\lambda\cdot D(Z_i, \bar{Z}))}}\\
\bar{Z} = \frac{1}{n}\sum_{i=1}^{n}{Z_i}\\
\text{where $n$ is augmented results number, $\bar{Z}$ is the mean of representations of different augmented
patches ,}\\
\text{In the TCR loss, Î» is set to 200.0 and $\epsilon^2$is set to 0.2 (Exp setting)} 
$$





## **Experiment**

> ablation study çœ‹é‚£ä¸ªæ¨¡å—æœ‰æ•ˆï¼Œæ€»ç»“ä¸€ä¸‹

We use a ResNet-18 [27] as the backbone and train for at most 30 epochs. We use a batch size of 100, the LARS optimizer [51] with Î· set to 0.005, and a weight decay of 1e-4.  In the TCR loss, Î» is set to 200.0 and  2 is set to 0.2

### CIFAR-100 Cls

Accuracy is measured by **training linear classifier on learned embedding representation**

åˆ†ç±»å‡†ç¡®ç‡æŒ‡æ ‡ & æ”¶æ•›é€Ÿåº¦å‡æœ€ä¼˜ï¼Œå¾ˆéœ‡æ’¼ :zap:

<table><tr>
    <td><img src='./docs/EMP-SSL_CIFAR_quantitative_result.png'></td>
    <td><img src='./docs/EMP-SSL_CIFAR_convergence_plot.png'></td>
</table>



Several Fining:

1. EMP-SSL 10 ä¸ª Epoch æ•ˆæœï¼Œä¼˜äºå…¶ä»– SOTA SSL æ–¹æ³• 1000 ä¸ª epochçš„ç»“æœ
2. patches æ•°é‡å¢åŠ   $20 \rightarrow 200$ æ•ˆæœæœ‰æ›´ä¼˜



### ablation study

- **number of patches $n$** to illustrate the importance of patch number in joint-embedding SSL

  patches =100 å°±å¾ˆå¥½äº†

- Transferability ä½†éƒ½è¿˜æ˜¯åˆ†ç±»ä»»åŠ¡åªæ˜¯æ•°æ®é›†ä¸ä¸€æ ·

  > Note that despite similar names, CIFAR-10 and CIFAR-100 have very little overlap hence they are suitable for testing modelâ€™s transferability.

  the methodâ€™s better transferability to out of domain data

  (1) models pretrained on CIFAR-10 and linearly evaluated on CIFAR-100 (2) models pretrained on CIFAR-100 and linearly evaluated on CIFAR-10

  1. SOTA SSL æ–¹æ³•åœ¨ä¸€ä¸ªä¸Šé¢å¥½æ¢æˆå¦ä¸€ä¸ªå°±ä¸è¡Œï¼Œ**è¯´æ˜ SSL çš„ latent representation éœ€è¦ generalize well to out-of-domain data instead of overfitting the training data**
  2. a larger number of training epochs causes the models to overfit to the training dataset.

  



## **Limitations**

## **Summary :star2:**

> learn what & how to apply to our task

- **learning more discriminative latent code like generative models** :star:

  æ€è€ƒæ€ä¹ˆæ”¹è¿›ç”¨åˆ° diffusion é‡Œé¢ï¼Œå‚è€ƒ [related works å†™çš„é‚£ä¸ªæ–‡ç« ](#Related Work)

  å®ç° diffusion åŠ é€Ÿ

