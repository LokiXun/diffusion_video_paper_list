# Stable Diffusion :moyai: (High-Resolution Image Synthesis with Latent Diffusion Models )



> [2022_CVPR_High-Resolution Image Synthesis with Latent Diffusion Models.pdf](./2022_CVPR_High-Resolution Image Synthesis with Latent Diffusion Models.pdf)
> [github](https://github.com/CompVis/stable-diffusion) ![GitHub Repo stars](https://img.shields.io/github/stars/CompVis/stable-diffusion?style=social)
> [Youtube åšä¸»è§£è¯»](https://www.youtube.com/watch?v=f6PtJKdey8E) [çŸ¥ä¹Žåšå®¢](https://zhuanlan.zhihu.com/p/597247221) [towardScience åšå®¢](https://towardsdatascience.com/paper-explained-high-resolution-image-synthesis-with-latent-diffusion-models-f372f7636d42) :+1:
> [What can Stable Diffusion do?](https://stable-diffusion-art.com/how-stable-diffusion-work/)
> [Stable Diffusion with ðŸ§¨ Diffusers](https://huggingface.co/blog/stable_diffusion#how-does-stable-diffusion-work)





## **KeyPoint**



**Contributions**

- å¤§å¹…é™ä½Žè®­ç»ƒå’Œé‡‡æ ·é˜¶æ®µçš„è®¡ç®—å¤æ‚åº¦ï¼Œè®©æ–‡å›¾ç”Ÿæˆç­‰ä»»åŠ¡èƒ½å¤Ÿåœ¨æ¶ˆè´¹çº§GPUä¸Šï¼Œåœ¨10ç§’çº§åˆ«æ—¶é—´ç”Ÿæˆå›¾ç‰‡ï¼Œå¤§å¤§é™ä½Žäº†è½åœ°é—¨æ§›



## **Related Work**

image generation has been tackled mainly through **four families of models**: Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), Autoregressive Models (ARMs), Diffusion Probabilistic Models(DMs)

- GANs: show promising results for **data with limited variability**, bearing mode-collapse, **unstable training**

  > **Mode collapse**: *this phenomenon occurs when the generator can alternately generate a limited number of outputs that fool the discriminator. In general, GANs **struggle capturing the full data distribution.***

- VAE(Variational Autoencoders)

  not suffer from mode-collapse and can efficiently generate high-resolution images. **sample quality** is **not** always **comparable to** that of **GANs**.

- DPM

  operation in **pixel space** by adding or removing noise to a tensor of the same size as the original image results in **slow inference speed** and **high computational cost**

## **methods**

> - :question:  VAE?
> - KL-reg, VQ-reg

`High-Resolution Image Synthesis with Latent Diffusion Models`, we can break it down into **four main steps** :star:

### Perceptual Image Compression

**Latent Diffusion explicitly separates the image compression** phase to **remove high frequency details (perceptual compression**), and learns a semantic and conceptual composition of the data (**semantic compression**).

> åŽŸæœ‰çš„éžæ„ŸçŸ¥åŽ‹ç¼©çš„æ‰©æ•£æ¨¡åž‹æœ‰ä¸€ä¸ªå¾ˆå¤§çš„é—®é¢˜åœ¨äºŽï¼Œç”±äºŽåœ¨åƒç´ ç©ºé—´ä¸Šè®­ç»ƒæ¨¡åž‹ï¼Œå¦‚æžœæˆ‘ä»¬å¸Œæœ›ç”Ÿæˆä¸€å¼ åˆ†è¾¨çŽ‡å¾ˆé«˜çš„å›¾ç‰‡ï¼Œè¿™å°±æ„å‘³ç€æˆ‘ä»¬è®­ç»ƒçš„ç©ºé—´ä¹Ÿæ˜¯ä¸€ä¸ªå¾ˆé«˜ç»´çš„ç©ºé—´ã€‚
> å¼•å…¥æ„ŸçŸ¥åŽ‹ç¼©å°±æ˜¯è¯´**é€šè¿‡ VAE è¿™ç±»è‡ªç¼–ç æ¨¡åž‹å¯¹åŽŸå›¾ç‰‡è¿›è¡Œå¤„ç†ï¼Œå¿½ç•¥æŽ‰å›¾ç‰‡ä¸­çš„é«˜é¢‘ä¿¡æ¯**ï¼Œåªä¿ç•™é‡è¦ã€åŸºç¡€çš„ä¸€äº›ç‰¹å¾ã€‚
>
> æ„ŸçŸ¥åŽ‹ç¼©ä¸»è¦åˆ©ç”¨ä¸€ä¸ªé¢„è®­ç»ƒçš„è‡ªç¼–ç æ¨¡åž‹ï¼Œè¯¥æ¨¡åž‹èƒ½å¤Ÿå­¦ä¹ åˆ°ä¸€ä¸ªåœ¨**æ„ŸçŸ¥ä¸Šç­‰åŒäºŽå›¾åƒç©ºé—´çš„æ½œåœ¨è¡¨ç¤ºç©ºé—´ latent space**ã€‚ä¼˜åŠ¿æ˜¯åªéœ€è¦è®­ç»ƒä¸€ä¸ªé€šç”¨çš„è‡ªç¼–ç æ¨¡åž‹ï¼Œå°±å¯ä»¥ç”¨äºŽä¸åŒçš„æ‰©æ•£æ¨¡åž‹çš„è®­ç»ƒï¼Œåœ¨ä¸åŒçš„ä»»åŠ¡ä¸Šï¼ˆimage2imageï¼Œtext2imageï¼‰ä½¿ç”¨ã€‚
>
> **åŸºäºŽæ„ŸçŸ¥åŽ‹ç¼©çš„æ‰©æ•£æ¨¡åž‹çš„è®­ç»ƒ**æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªä¸¤é˜¶æ®µè®­ç»ƒçš„è¿‡ç¨‹ï¼Œç¬¬ä¸€é˜¶æ®µéœ€è¦è®­ç»ƒä¸€ä¸ªè‡ªç¼–ç å™¨ï¼Œç¬¬äºŒé˜¶æ®µæ‰éœ€è¦è®­ç»ƒæ‰©æ•£æ¨¡åž‹æœ¬èº«ã€‚
>
> $\mathcal{x} \in R^{H\times W \times 3} \rightarrow R^{h\times w \times 3} $ï¼Œä¸‹é‡‡æ ·å› å­ $f = H/h = W/w = 2^m$ 
>
> - åŽ‹ç¼©æ¯”çš„é€‰å–ï¼Œæ¯”è¾ƒ FIDï¼ŒInception Scores vs. training progress



#### Optimization

![VQGAN_approach_overview.jpg](C:\Users\Loki\workspace\LearningJourney_Notes\Tongji_CV_group\docs\VQGAN_approach_overview.jpg)

- Regularization

  - KL(Kullback-Leibler) æ•£åº¦ [åšå®¢å‚è€ƒ](https://zhuanlan.zhihu.com/p/100676922)

    **è®¡ç®—ä¸¤ä¸ªæ¦‚çŽ‡åˆ†å¸ƒä¹‹é—´å·®å¼‚**ï¼Œå½“ç”¨ä¸€ä¸ªåŽ»è¿‘ä¼¼å¦ä¸€ä¸ªåˆ†å¸ƒï¼Œè®¡ç®—ä¼šæœ‰å¤šå°‘æŸå¤±ã€‚ps: KL æ•£åº¦å¾ˆåƒè·ç¦»ï¼Œä½†ä¸æ˜¯ï¼Œå› ä¸º :Loss_KL(p|q) != Loss_KL(q|p) ä¸å¯¹ç§°ï¼





### Latent Diffusion Models

LDM åœ¨è®­ç»ƒæ—¶å°±å¯ä»¥åˆ©ç”¨ç¼–ç å™¨å¾—åˆ° $z_t$
$$
L_{DM} := E_{\epsilon(x),\varepsilon\sim\mathcal{N}(0,1),t}{[\abs{\abs{ \epsilon - \epsilon_\theta(x_t,t)}}^2_2]}
\\
L_{LDM} := E_{\epsilon(x),\varepsilon\sim\mathcal{N}(0,1),t}{[\abs{\abs{ \epsilon - \epsilon_\theta(z_t,t)}}^2_2]}
$$


### Conditioning Mechanisms





**Limitations**



**Summary**

> learn what & how to apply to our task

# Code >> Stable Diffusion

> pytorch-lighting  >> save moves for zero the gradient,...
>
> https://zhuanlan.zhihu.com/p/613337342

```
# training config
configs/autoencoder/autoencoder_kl_64x64x3.yaml  # dataset image->256x256x3
configs/latent-diffusion/cin-ldm-vq-f8.yaml
```

- Loading module from yaml info >> use same code to initialize different classes

  ```python
  import importlib
  
  """ yaml example
      unet_config:
        target: ldm.modules.diffusionmodules.openaimodel.UNetModel
        params:
          image_size: 32
  """
  
  def instantiate_from_config(config):
      if not "target" in config:
          if config == '__is_first_stage__':
              return None
          elif config == "__is_unconditional__":
              return None
          raise KeyError("Expected key `target` to instantiate.")
      return get_obj_from_str(config["target"])(**config.get("params", dict()))
  
  
  def get_obj_from_str(string, reload=False):
      module, cls = string.rsplit(".", 1)
      if reload:
          module_imp = importlib.import_module(module)
          importlib.reload(module_imp)
      return getattr(importlib.import_module(module, package=None), cls)
  ```

  

## Autoencoder

`def training_step` main training procedures happens

> VQGAN code learning

- **LPIPS Loss >> perceptual loss**

  input & reconstruction from Autoencoder all shapes like [B, 3, 256, 256]

  use **pretrained `VGG16`'s feature Module**, which is `nn.Sequential` hence the layer could be visited by index. What `LPIPS` do is **split the starting 30 layers into 5 slice `nn.Sequential`** (cut at layer indexing by 3,8,15,22,29 and **each module ends with layer `RELU(inplace=True)`**)

  ```python
  from torchvision import models
  
  vgg_pretrained_features = models.vgg16(pretrained=pretrained).features  # nn.Sequential 31 layers
  # [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M']
  # number-> ConvModule(conv2d+relu) "M"->Maxpooling
  
  # output with 5 moudle's output (each end with RELU layer)
  vgg_outputs = namedtuple("VggOutputs", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3', 'relu5_3'])
  ```

  > What could we learn from this LPIPS implementationï¼Ÿ
  >
  > **tear pretrained Module apart to get intermediate layers output** 
  >
  > ```python
  > class vgg16(torch.nn.Module):
  >     def __init__(self, requires_grad=False, pretrained=True):
  >         super(vgg16, self).__init__()
  >         vgg_pretrained_features = models.vgg16(pretrained=pretrained).features  # nn.Sequential hence could be visit by index 
  >         self.slice1 = torch.nn.Sequential()
  >         for x in range(4):
  >             self.slice1.add_module(str(x), vgg_pretrained_features[x]) 
  >             
  >         # ...
  > 	def forward(self, X): 
  >         h = self.slice1(X)
  >         h_relu1_2 = h
  >         # ...
  >         vgg_outputs = namedtuple("VggOutputs", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3', 'relu5_3'])
  >         return vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3, h_relu5_3)
  >   
  > ```

  

- download file standard ways

  ```python
  def download(url, local_path, chunk_size=1024):
      os.makedirs(os.path.split(local_path)[0], exist_ok=True)
      with requests.get(url, stream=True) as r:
          total_size = int(r.headers.get("content-length", 0))
          with tqdm(total=total_size, unit="B", unit_scale=True) as pbar:
              with open(local_path, "wb") as f:
                  for data in r.iter_content(chunk_size=chunk_size):
                      if data:
                          f.write(data)
                          pbar.update(chunk_size)
  ```

- import module from string in config file

  ```python
  import importlib
  
  def get_obj_from_str(string, reload=False):
      module, cls = string.rsplit(".", 1)
      if reload:
          module_imp = importlib.import_module(module)
          importlib.reload(module_imp)
      return getattr(importlib.import_module(module, package=None), cls)
  ```

  

- KL loss

  > [How to generate Gaussian samples](https://medium.com/mti-technology/how-to-generate-gaussian-samples-347c391b7959)
  >
  > - Variance æ–¹å·®
  > - Standard Deviation
  >   Standard deviation is a statistic that measures the dispersion of a dataset relative to its [mean](https://www.investopedia.com/terms/m/mean.asp) and is calculated as the square root of the [variance](https://www.investopedia.com/terms/v/variance.asp). 
  > - KL divergence [blog](https://zhuanlan.zhihu.com/p/438129018)
  
  `class DiagonalGaussianDistribution` >> `ldm/modules/distributions/distributions.py`
  
  - sample
  
    `logvar` >> $\log{(\sigma^2)}$
    $$
    \text{assume } z\sim\mathcal{N}(0,1) \\
    if \\
    x = \mu + \sigma * Z = \mu + exp(0.5 * \log{(\sigma^2)}) * Z \\
    \text{lets say } x\sim\mathcal{N}(\mu, \sigma)\\
    $$
  
  - KL divergence
    $$
    KL\_Loss = 0.5 *\sum{(\mu^2 + \sigma^2 - 1 - \log{\sigma^2})}
    $$
  
  - VQ-GAN adaptive weight
    $$
    Loss = \mathcal{L}_{resc} +\lambda * \mathcal{L}_{GAN} \\
    \lambda = \frac{\nabla_{GL}{\mathcal{L}_{resc}}}{\nabla_{GL}{\mathcal{L}_{GAN}} +1e-4}
    $$
    **èŽ·å–æœ€åŽä¸€å±‚ layer çš„æ¢¯åº¦**
  
    ```
    last_layer = self.decoder.last_layer.weight
    
    nll_grads = torch.autograd.grad(nll_loss, last_layer, retain_graph=True)[0]
    ```
  
- PatchGAN discriminator

  > [Question: PatchGAN Discriminator](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/39)
  >
  > In fact, a "PatchGAN" is just a convnet! Or you could say all convnets are patchnets: the power of convnets is that they process each image patch identically and independently. å·ç§¯æ ¸å¤„ç†çš„æ—¶å€™å°±æ˜¯ä¸€ä¸ª image patch

  

## LatentDiffusion Train

>  in `ldm/models/diffusion/ddpm.py`
>  configure file=`configs/latent-diffusion/cin-ldm-vq-f8.yaml`
>

initialize the `class DDPM(pl.LightningModule)`

### UNetModel

> U-net model in `self.model = DiffusionWrapper(unet_config, conditioning_key)`
> `class UNetModel(nn.Module):`  >> `ldm/modules/diffusionmodules/openaimodel.py`
>
> *The full UNet model with attention and timestep embedding.* 
> See U-net structure >> [Stable-diffusion_U-net-structure-with-Note.pdf](Stable-diffusion_U-net-structure-with-Note.pdf)
>
> - [What is the 'zero_module' used for?](https://github.com/openai/guided-diffusion/issues/21)
>
>    is used to initialize certain modules to zero. 
>
>   ```python
>   def zero_module(module):
>       """
>       Zero out the parameters of a module and return it. >> Initialize the module with 0
>       """
>       for p in module.parameters():
>           p.detach().zero_()
>       return module
>   ```

![U-net structure](./docs/stable_diffusion_unet_architecture.png)



#### **timestep_embedding**

> `ldm/modules/diffusionmodules/util.py`
> [timestep embedding blog](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/#what-is-positional-encoding-and-why-do-we-need-it-in-the-first-place) :star:
> [detailed proof for]

**basics**

the time-embedding should satisfy the following criteria:

1. unique each  timestep(word's position)
2. distance between 2 time-step should be consistent across different sentence
3. generalize to longer sentences
4. determinstic

using binary values would be a waste of space in the world of floats. So instead, we can use their float continous counterparts - **Sinusoidal functions. Indeed, they are the equivalent to alternating bits.** 

<figure>
    <img src="https://d33wubrfki0l68.cloudfront.net/ef81ee3018af6ab6f23769031f8961afcdd67c68/3358f/img/transformer_architecture_positional_encoding/positional_encoding.png">
    <figcaption> 
        <center>
        <I>Sinusoidal embedding for dim=50
        </center>
    </figcaption>
</figure>



***sinusoidal timestep embeddings.***

$POS_{i} * POS_{i+k}$ éšç€é—´éš” K å¢žå¤§ï¼Œé€æ¸å‡å°æ¥è¡¨ç¤ºç›¸å¯¹ä½ç½®å…³ç³»ã€‚ä½†å†…ç§¯æ˜¯å¯¹ç§°çš„ï¼Œæ— æ³•åŒºåˆ†å‰åŽå…³ç³»

```python
freqs = torch.exp(-math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half).to(device=timesteps.device)
args = timesteps[:, None].float() * freqs[None]  # [N,1] * [1, half] >> [N, half]
embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)  # [N, dim]

time_embed_dim = model_channels * 4
time_embed = nn.Sequential(
            linear(model_channels, time_embed_dim),
            nn.SiLU(),
            linear(time_embed_dim, time_embed_dim),
        )
```



**LitEma**

æ»‘åŠ¨å¹³å‡



**register_schedule**

> Whole bunch of formulations could be check at [DDPM video 42:19](https://www.youtube.com/watch?v=y7J6sSO1k50)

- beta >> linear 
  `torch.linspace(linear_start ** 0.5, linear_end ** 0.5, n_timestep, dtype=torch.float64) ** 2`

- alpha $[1, 0.9999, \cdots, 7e-4]$

- $\bar{\alpha_i}$ 

  ```
  alphas_cumprod = np.cumprod(alphas, axis=0)
  alphas_cumprod_prev = np.append(1., alphas_cumprod[:-1])
  ```



**VQModelInterface**

> `ldm.models.autoencoder.VQModelInterface`



> Summary
>
> æœ‰äº† x å’Œ condition info ( class_name )
>
> 1. å°† x é€šè¿‡ VQGAN encoder æ˜ å°„ä¸º latent code
>
> 2. DDPM è®­ç»ƒ
>
>    1. ä»Ž 0-1000 éšæœºå– timestep 
>
>    2. condition  ç»è¿‡ `ClassEmbedder `æ˜ å°„å¾—åˆ° 1x512 tensor
>
>    3. `def p_losses` ä»Žå¹²å‡€ x0 åŠ å™ª T æ­¥
>
>       éšæœºå–å™ªå£° noise $\epsilon$ (shape å’Œ x0 ä¸€æ ·)
>
>       `def q_sample` æŒ‰å…¬å¼åŠ å™ª $q(x_t | x_0)\sim \mathcal{N}(\sqrt{\bar{a_t}} x_0, (1-\bar{a_t})I)$
>
>    4. è°ƒ U-net è¾“å…¥ x_t, t, condition é¢„æµ‹å™ªå£°ï¼Œä¸Žä¹‹å‰éšæœºå–çš„ noise $\epsilon$ è®¡ç®— L2 loss



## Text2Image

> `scripts/txt2img.py`

### PLMS

> [PNDM github repo](https://github.com/luping-liu/PNDM)
> [arXiv: Pseudo Numerical Methods for Diffusion Models on Manifolds](https://arxiv.org/abs/2202.09778)
> [local pdf](./2022_ICLR_PNDM_Pseudo-Numerical-Methods-for-Diffusion-Models-on-Manifolds.pdf)

- [invisible-watermark](https://github.com/ShieldMnt/invisible-watermark)
