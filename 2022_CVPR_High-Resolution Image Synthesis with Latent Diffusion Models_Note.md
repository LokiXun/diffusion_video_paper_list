# High-Resolution Image Synthesis with Latent Diffusion Models (stable diffusion)

> [github](https://github.com/CompVis/stable-diffusion) ![GitHub Repo stars](https://img.shields.io/github/stars/CompVis/stable-diffusion?style=social)
> [pdf](./2022_CVPR_High-Resolution Image Synthesis with Latent Diffusion Models.pdf)
> [Youtube Âçö‰∏ªËß£ËØª](https://www.youtube.com/watch?v=f6PtJKdey8E) [Áü•‰πéÂçöÂÆ¢](https://zhuanlan.zhihu.com/p/597247221) [towardScience ÂçöÂÆ¢](https://towardsdatascience.com/paper-explained-high-resolution-image-synthesis-with-latent-diffusion-models-f372f7636d42) :+1:
> [What can Stable Diffusion do?](https://stable-diffusion-art.com/how-stable-diffusion-work/)
> [Stable Diffusion with üß® Diffusers](https://huggingface.co/blog/stable_diffusion#how-does-stable-diffusion-work)



## **KeyPoint**

**Contributions**

- Â§ßÂπÖÈôç‰ΩéËÆ≠ÁªÉÂíåÈááÊ†∑Èò∂ÊÆµÁöÑËÆ°ÁÆóÂ§çÊùÇÂ∫¶ÔºåËÆ©ÊñáÂõæÁîüÊàêÁ≠â‰ªªÂä°ËÉΩÂ§üÂú®Ê∂àË¥πÁ∫ßGPU‰∏äÔºåÂú®10ÁßíÁ∫ßÂà´Êó∂Èó¥ÁîüÊàêÂõæÁâáÔºåÂ§ßÂ§ßÈôç‰Ωé‰∫ÜËêΩÂú∞Èó®Êßõ



## **Related Work**

image generation has been tackled mainly through **four families of models**: Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), Autoregressive Models (ARMs), Diffusion Probabilistic Models(DMs)

- GANs: show promising results for **data with limited variability**, bearing mode-collapse, **unstable training**

  > **Mode collapse**: *this phenomenon occurs when the generator can alternately generate a limited number of outputs that fool the discriminator. In general, GANs **struggle capturing the full data distribution.***

- VAE(Variational Autoencoders)

  not suffer from mode-collapse and can efficiently generate high-resolution images. **sample quality** is **not** always **comparable to** that of **GANs**.

- DPM

  operation in **pixel space** by adding or removing noise to a tensor of the same size as the original image results in **slow inference speed** and **high computational cost**

## **methods**

> - :question:  VAE?
> - KL-reg, VQ-reg

`High-Resolution Image Synthesis with Latent Diffusion Models`, we can break it down into **four main steps** :star:

### Perceptual Image Compression

**Latent Diffusion explicitly separates the image compression** phase to **remove high frequency details (perceptual compression**), and learns a semantic and conceptual composition of the data (**semantic compression**).

> ÂéüÊúâÁöÑÈùûÊÑüÁü•ÂéãÁº©ÁöÑÊâ©Êï£Ê®°ÂûãÊúâ‰∏Ä‰∏™ÂæàÂ§ßÁöÑÈóÆÈ¢òÂú®‰∫éÔºåÁî±‰∫éÂú®ÂÉèÁ¥†Á©∫Èó¥‰∏äËÆ≠ÁªÉÊ®°ÂûãÔºåÂ¶ÇÊûúÊàë‰ª¨Â∏åÊúõÁîüÊàê‰∏ÄÂº†ÂàÜËæ®ÁéáÂæàÈ´òÁöÑÂõæÁâáÔºåËøôÂ∞±ÊÑèÂë≥ÁùÄÊàë‰ª¨ËÆ≠ÁªÉÁöÑÁ©∫Èó¥‰πüÊòØ‰∏Ä‰∏™ÂæàÈ´òÁª¥ÁöÑÁ©∫Èó¥„ÄÇ
> ÂºïÂÖ•ÊÑüÁü•ÂéãÁº©Â∞±ÊòØËØ¥**ÈÄöËøá VAE ËøôÁ±ªËá™ÁºñÁ†ÅÊ®°ÂûãÂØπÂéüÂõæÁâáËøõË°åÂ§ÑÁêÜÔºåÂøΩÁï•ÊéâÂõæÁâá‰∏≠ÁöÑÈ´òÈ¢ë‰ø°ÊÅØ**ÔºåÂè™‰øùÁïôÈáçË¶Å„ÄÅÂü∫Á°ÄÁöÑ‰∏Ä‰∫õÁâπÂæÅ„ÄÇ
>
> ÊÑüÁü•ÂéãÁº©‰∏ªË¶ÅÂà©Áî®‰∏Ä‰∏™È¢ÑËÆ≠ÁªÉÁöÑËá™ÁºñÁ†ÅÊ®°ÂûãÔºåËØ•Ê®°ÂûãËÉΩÂ§üÂ≠¶‰π†Âà∞‰∏Ä‰∏™Âú®**ÊÑüÁü•‰∏äÁ≠âÂêå‰∫éÂõæÂÉèÁ©∫Èó¥ÁöÑÊΩúÂú®Ë°®Á§∫Á©∫Èó¥ latent space**„ÄÇ‰ºòÂäøÊòØÂè™ÈúÄË¶ÅËÆ≠ÁªÉ‰∏Ä‰∏™ÈÄöÁî®ÁöÑËá™ÁºñÁ†ÅÊ®°ÂûãÔºåÂ∞±ÂèØ‰ª•Áî®‰∫é‰∏çÂêåÁöÑÊâ©Êï£Ê®°ÂûãÁöÑËÆ≠ÁªÉÔºåÂú®‰∏çÂêåÁöÑ‰ªªÂä°‰∏äÔºàimage2imageÔºåtext2imageÔºâ‰ΩøÁî®„ÄÇ
>
> **Âü∫‰∫éÊÑüÁü•ÂéãÁº©ÁöÑÊâ©Êï£Ê®°ÂûãÁöÑËÆ≠ÁªÉ**Êú¨Ë¥®‰∏äÊòØ‰∏Ä‰∏™‰∏§Èò∂ÊÆµËÆ≠ÁªÉÁöÑËøáÁ®ãÔºåÁ¨¨‰∏ÄÈò∂ÊÆµÈúÄË¶ÅËÆ≠ÁªÉ‰∏Ä‰∏™Ëá™ÁºñÁ†ÅÂô®ÔºåÁ¨¨‰∫åÈò∂ÊÆµÊâçÈúÄË¶ÅËÆ≠ÁªÉÊâ©Êï£Ê®°ÂûãÊú¨Ë∫´„ÄÇ
>
> $\mathcal{x} \in R^{H\times W \times 3} \rightarrow R^{h\times w \times 3} $Ôºå‰∏ãÈááÊ†∑Âõ†Â≠ê $f = H/h = W/w = 2^m$ 
>
> - ÂéãÁº©ÊØîÁöÑÈÄâÂèñÔºåÊØîËæÉ FIDÔºåInception Scores vs. training progress



#### Optimization

> Details in Appendix G

We train all our autoencoder models in an adversarial manner following VQGAN

![VQGAN_approach_overview.jpg](./docs\VQGAN_approach_overview.jpg)

- Regularization

  - KL(Kullback-Leibler) Êï£Â∫¶ [ÂçöÂÆ¢ÂèÇËÄÉ](https://zhuanlan.zhihu.com/p/100676922)

    **ËÆ°ÁÆó‰∏§‰∏™Ê¶ÇÁéáÂàÜÂ∏É‰πãÈó¥Â∑ÆÂºÇ**ÔºåÂΩìÁî®‰∏Ä‰∏™ÂéªËøë‰ººÂè¶‰∏Ä‰∏™ÂàÜÂ∏ÉÔºåËÆ°ÁÆó‰ºöÊúâÂ§öÂ∞ëÊçüÂ§±„ÄÇps: KL Êï£Â∫¶ÂæàÂÉèË∑ùÁ¶ªÔºå‰ΩÜ‰∏çÊòØÔºåÂõ†‰∏∫ :Loss_KL(p|q) != Loss_KL(q|p) ‰∏çÂØπÁß∞ÔºÅ





### Latent Diffusion Models

LDM Âú®ËÆ≠ÁªÉÊó∂Â∞±ÂèØ‰ª•Âà©Áî®ÁºñÁ†ÅÂô®ÂæóÂà∞ $z_t$
$$
L_{DM} := E_{\epsilon(x),\varepsilon\sim\mathcal{N}(0,1),t}{[\abs{\abs{ \epsilon - \epsilon_\theta(x_t,t)}}^2_2]}
\\
L_{LDM} := E_{\epsilon(x),\varepsilon\sim\mathcal{N}(0,1),t}{[\abs{\abs{ \epsilon - \epsilon_\theta(z_t,t)}}^2_2]}
$$


### Conditioning Mechanisms





**Limitations**



**Summary**

> learn what & how to apply to our task

# Code

> pytorch-lighting  >> save moves for zero the gradient,...
> [stable-diffusion code analysis blog](https://zhuanlan.zhihu.com/p/613337342)
>
> - Summary
>
>   Êúâ‰∫Ü x Âíå condition info ( class_name )
>
>   1. Â∞Ü x ÈÄöËøá VQGAN encoder Êò†Â∞Ñ‰∏∫ latent code
>
>   2. DDPM ËÆ≠ÁªÉ
>
>      1. ‰ªé 0-1000 ÈöèÊú∫Âèñ timestep 
>
>      2. condition  ÁªèËøá `ClassEmbedder `Êò†Â∞ÑÂæóÂà∞ 1x512 tensor
>
>      3. `def p_losses` ‰ªéÂπ≤ÂáÄ x0 Âä†Âô™ T Ê≠•
>
>         ÈöèÊú∫ÂèñÂô™Â£∞ noise $\epsilon$ (shape Âíå x0 ‰∏ÄÊ†∑)
>
>         `def q_sample` ÊåâÂÖ¨ÂºèÂä†Âô™ $q(x_t | x_0)\sim \mathcal{N}(\sqrt{\bar{a_t}} x_0, (1-\bar{a_t})I)$
>
>      4. Ë∞É U-net ËæìÂÖ• x_t, t, condition È¢ÑÊµãÂô™Â£∞Ôºå‰∏é‰πãÂâçÈöèÊú∫ÂèñÁöÑ noise $\epsilon$ ËÆ°ÁÆó L2 loss

```
# training config
configs/autoencoder/autoencoder_kl_64x64x3.yaml  # dataset image->256x256x3
configs/latent-diffusion/cin-ldm-vq-f8.yaml
```

- Loading module from yaml info >> use same code to initialize different classes

  ```python
  import importlib
  
  """ yaml example
      unet_config:
        target: ldm.modules.diffusionmodules.openaimodel.UNetModel
        params:
          image_size: 32
  """
  
  def instantiate_from_config(config):
      if not "target" in config:
          if config == '__is_first_stage__':
              return None
          elif config == "__is_unconditional__":
              return None
          raise KeyError("Expected key `target` to instantiate.")
      return get_obj_from_str(config["target"])(**config.get("params", dict()))
  
  
  def get_obj_from_str(string, reload=False):
      module, cls = string.rsplit(".", 1)
      if reload:
          module_imp = importlib.import_module(module)
          importlib.reload(module_imp)
      return getattr(importlib.import_module(module, package=None), cls)
  ```

  

## AutoencoderKL

`def training_step` main training procedures happens

> VQGAN code learning

- **LPIPS Loss >> perceptual loss**

  input & reconstruction from Autoencoder all shapes like [B, 3, 256, 256]

  use **pretrained `VGG16`'s feature Module**, which is `nn.Sequential` hence the layer could be visited by index. What `LPIPS` do is **split the starting 30 layers into 5 slice `nn.Sequential`** (cut at layer indexing by 3,8,15,22,29 and **each module ends with layer `RELU(inplace=True)`**)

  ```python
  from torchvision import models
  
  vgg_pretrained_features = models.vgg16(pretrained=pretrained).features  # nn.Sequential 31 layers
  # [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M']
  # number-> ConvModule(conv2d+relu) "M"->Maxpooling
  
  # output with 5 moudle's output (each end with RELU layer)
  vgg_outputs = namedtuple("VggOutputs", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3', 'relu5_3'])
  ```

  > What could we learn from this LPIPS implementationÔºü
  >
  > **tear pretrained Module apart to get intermediate layers output** 
  >
  > ```python
  > class vgg16(torch.nn.Module):
  >     def __init__(self, requires_grad=False, pretrained=True):
  >         super(vgg16, self).__init__()
  >         vgg_pretrained_features = models.vgg16(pretrained=pretrained).features  # nn.Sequential hence could be visit by index 
  >         self.slice1 = torch.nn.Sequential()
  >         for x in range(4):
  >             self.slice1.add_module(str(x), vgg_pretrained_features[x]) 
  >             
  >         # ...
  > 	def forward(self, X): 
  >         h = self.slice1(X)
  >         h_relu1_2 = h
  >         # ...
  >         vgg_outputs = namedtuple("VggOutputs", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3', 'relu5_3'])
  >         return vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3, h_relu5_3)
  >   
  > ```

  

- download file standard ways

  ```python
  def download(url, local_path, chunk_size=1024):
      os.makedirs(os.path.split(local_path)[0], exist_ok=True)
      with requests.get(url, stream=True) as r:
          total_size = int(r.headers.get("content-length", 0))
          with tqdm(total=total_size, unit="B", unit_scale=True) as pbar:
              with open(local_path, "wb") as f:
                  for data in r.iter_content(chunk_size=chunk_size):
                      if data:
                          f.write(data)
                          pbar.update(chunk_size)
  ```

- import module from string in config file

  ```python
  import importlib
  
  def get_obj_from_str(string, reload=False):
      module, cls = string.rsplit(".", 1)
      if reload:
          module_imp = importlib.import_module(module)
          importlib.reload(module_imp)
      return getattr(importlib.import_module(module, package=None), cls)
  ```

  

- KL loss

  > [How to generate Gaussian samples](https://medium.com/mti-technology/how-to-generate-gaussian-samples-347c391b7959)
  >
  > - Variance ÊñπÂ∑Æ
  > - Standard Deviation
  >   Standard deviation is a statistic that measures the dispersion of a dataset relative to its [mean](https://www.investopedia.com/terms/m/mean.asp) and is calculated as the square root of the [variance](https://www.investopedia.com/terms/v/variance.asp). 
  > - KL divergence [blog](https://zhuanlan.zhihu.com/p/438129018)
  
  `class DiagonalGaussianDistribution` >> `ldm/modules/distributions/distributions.py`
  
  - sample
  
    `logvar` >> $\log{(\sigma^2)}$
    $$
    \text{assume } z\sim\mathcal{N}(0,1) \\
    if \\
    x = \mu + \sigma * Z = \mu + exp(0.5 * \log{(\sigma^2)}) * Z \\
    \text{lets say } x\sim\mathcal{N}(\mu, \sigma)\\
    $$
  
  - KL divergence
    $$
    KL\_Loss = 0.5 *\sum{(\mu^2 + \sigma^2 - 1 - \log{\sigma^2})}
    $$
  
  - VQ-GAN adaptive weight
    $$
    Loss = \mathcal{L}_{resc} +\lambda * \mathcal{L}_{GAN} \\
    \lambda = \frac{\nabla_{GL}{\mathcal{L}_{resc}}}{\nabla_{GL}{\mathcal{L}_{GAN}} +1e-4}
    $$
    **Ëé∑ÂèñÊúÄÂêé‰∏ÄÂ±Ç layer ÁöÑÊ¢ØÂ∫¶**
  
    ```
    last_layer = self.decoder.last_layer.weight
    
    nll_grads = torch.autograd.grad(nll_loss, last_layer, retain_graph=True)[0]
    ```
  
- PatchGAN discriminator

  > [Question: PatchGAN Discriminator](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/39)
  >
  > In fact, a "PatchGAN" is just a convnet! Or you could say all convnets are patchnets: the power of convnets is that they process each image patch identically and independently. Âç∑ÁßØÊ†∏Â§ÑÁêÜÁöÑÊó∂ÂÄôÂ∞±ÊòØ‰∏Ä‰∏™ image patch

  

## LatentDiffusion Train

>  in `ldm/models/diffusion/ddpm.py`
>  configure file=`configs/latent-diffusion/cin-ldm-vq-f8.yaml`

initialize the `class DDPM(pl.LightningModule)`

### UNetModel

> U-net model in `self.model = DiffusionWrapper(unet_config, conditioning_key)`
> `class UNetModel(nn.Module):`  >> `ldm/modules/diffusionmodules/openaimodel.py`
>
> *The full UNet model with attention and timestep embedding.* 
> See U-net structure >> [Stable-diffusion_U-net-structure-with-Note.pdf](Stable-diffusion_U-net-structure-with-Note.pdf)
>
> - [What is the 'zero_module' used for?](https://github.com/openai/guided-diffusion/issues/21)
>
>    is used to initialize certain modules to zero. 
>
>   ```python
>   def zero_module(module):
>       """
>       Zero out the parameters of a module and return it. >> Initialize the module with 0
>       """
>       for p in module.parameters():
>           p.detach().zero_()
>       return module
>   ```

![U-net structure](./docs/stable_diffusion_unet_architecture.png)

> Upsample ‰∏ÄËà¨ÈÉΩÊòØÂÖà Upsample ÂÜçÊé• `Conv2d(kernel_size=3,stride=1,padding=1)`





#### **timestep_embedding**

> `ldm/modules/diffusionmodules/util.py`
> [timestep embedding blog](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/#what-is-positional-encoding-and-why-do-we-need-it-in-the-first-place) :star:
> [detailed proof for]

**basics**

the time-embedding should satisfy the following criteria:

1. unique each  timestep(word's position)
2. distance between 2 time-step should be consistent across different sentence
3. generalize to longer sentences
4. determinstic

using binary values would be a waste of space in the world of floats. So instead, we can use their float continous counterparts - **Sinusoidal functions. Indeed, they are the equivalent to alternating bits.** 

<figure>
    <img src="https://d33wubrfki0l68.cloudfront.net/ef81ee3018af6ab6f23769031f8961afcdd67c68/3358f/img/transformer_architecture_positional_encoding/positional_encoding.png">
    <figcaption> 
        <center>
        <I>Sinusoidal embedding for dim=50
        </center>
    </figcaption>
</figure>



***sinusoidal timestep embeddings.***

$POS_{i} * POS_{i+k}$ ÈöèÁùÄÈó¥Èöî K Â¢ûÂ§ßÔºåÈÄêÊ∏êÂáèÂ∞èÊù•Ë°®Á§∫Áõ∏ÂØπ‰ΩçÁΩÆÂÖ≥Á≥ª„ÄÇ‰ΩÜÂÜÖÁßØÊòØÂØπÁß∞ÁöÑÔºåÊó†Ê≥ïÂå∫ÂàÜÂâçÂêéÂÖ≥Á≥ª

```python
freqs = torch.exp(-math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half).to(device=timesteps.device)
args = timesteps[:, None].float() * freqs[None]  # [N,1] * [1, half] >> [N, half]
embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)  # [N, dim]

time_embed_dim = model_channels * 4
time_embed = nn.Sequential(
            linear(model_channels, time_embed_dim),
            nn.SiLU(),
            linear(time_embed_dim, time_embed_dim),
        )
```



**LitEma**

ÊªëÂä®Âπ≥Âùá



**register_schedule**

> Whole bunch of formulations could be check at [DDPM video 42:19](https://www.youtube.com/watch?v=y7J6sSO1k50)

- beta >> linear 
  `torch.linspace(linear_start ** 0.5, linear_end ** 0.5, n_timestep, dtype=torch.float64) ** 2`

- alpha $[1, 0.9999, \cdots, 7e-4]$

- $\bar{\alpha_i}$ 

  ```
  alphas_cumprod = np.cumprod(alphas, axis=0)
  alphas_cumprod_prev = np.append(1., alphas_cumprod[:-1])
  ```



## **VQModelInterface**

> `ldm.models.autoencoder.VQModelInterface`



## LitEMA :open_hands:

> [blog: stable-diffusion optimization: EMA weights on CPU](https://lunnova.dev/articles/stable-diffusion-ema-on-cpu/)
> Stable Diffusion ÁöÑ LitEMA ÊªëÂä®Âπ≥ÂùáÂõûÂà∞ÊòØÂèÇÊï∞Âú® GPU Êüê‰∏ÄÊó∂ÂàªÂä†ÂÄçÔºåÂèØËÉΩÂØºËá¥ÊòæÂ≠òÊ∫¢Âá∫ÔºÅÂèØ‰ª•Ê†πÊçÆÊ≠§ blog ‰øÆÊîπ `LitEMA()` Â∞ÜÂèÇÊï∞Â≠òÂú® CPU ‰∏äÔºåÊçÆËØ¥ÂèØ‰ª•‰ªé 32G -> 27G ÊòæÂ≠ò

Stable diffusion uses an Exponential Moving Average of the model's weights to improve quality of resulting images and **avoid overfitting to the most recently trained images.**

Stable Diffusion includes an implementation of an EMA called `LitEma`, found at [ldm/modules/ema.py](https://github.com/CompVis/stable-diffusion/blob/69ae4b35e0a0f6ee1af8bb9a5d0016ccb27e36dc/ldm/modules/ema.py)

- How do you implement an EMA for a machine learning model?

  With PyTorch modules you can use [the `named_parameters()` iterator](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_parameters) to access all parameters. :star:

- the EMA weights end up on the GPU like everything else. This **doubles the memory required** to store the model parameters! ÂèØËÉΩÈÄ†ÊàêÊòæÂ≠òÊ∫¢Âá∫ !! :warning:

  > [How to keep some LightningModule's parameters on cpu when using CUDA devices for training](https://github.com/Lightning-AI/lightning/issues/3698)



## Text2Image

> `scripts/txt2img.py`

### PLMS

> [PNDM github repo](https://github.com/luping-liu/PNDM)
> [arXiv: Pseudo Numerical Methods for Diffusion Models on Manifolds](https://arxiv.org/abs/2202.09778)
> [local pdf](./2022_ICLR_PNDM_Pseudo-Numerical-Methods-for-Diffusion-Models-on-Manifolds.pdf)

- [invisible-watermark](https://github.com/ShieldMnt/invisible-watermark)



## finetune SDv2-inpaint

> [diffuser baisc training](https://huggingface.co/docs/diffusers/tutorials/basic_training)
>
> SDv2-inpaint checkpoint https://huggingface.co/stabilityai/stable-diffusion-2-inpainting :star:

This `stable-diffusion-2-inpainting` model is resumed from [stable-diffusion-2-base](https://huggingface.co/stabilityai/stable-diffusion-2-base) (`512-base-ema.ckpt`) and trained for another 200k steps.

- QÔºöËæìÂÖ•Ôºü

```python
z, c, x, xrec, xc = super().get_input(batch, self.first_stage_key, return_first_stage_outputs=True,force_c_encode=True, return_original_cond=True, bs=bs)
# inpaint: [z: jpg_vae_fea, c:text_fea, x:jpg, xrec: vae_Recon, xc: text]
bchw = z.shape

c_cat = list()  # 
# mask, resize to z's shape
cc = torch.nn.functional.interpolate(cc, size=bchw[-2:])
c_cat.append(cc)
# masked_image, apply vae-encde; 
cc = self.get_first_stage_encoding(self.encode_first_stage(cc))
c_cat.append(cc)

c_cat = torch.cat(c_cat, dim=1)
all_conds = {"c_concat": [c_cat], "c_crossattn": [c]}
```

ÂàùÂßãÂô™Â£∞ xTÔºåËÆ≠ÁªÉÊó∂ÂÄôÁõ¥Êé•ÂØπ GT Âä†Âô™

```
prng = np.random.RandomState(seed)
start_code = prng.randn(num_samples, 4, h // 8, w // 8)
start_code = torch.from_numpy(start_code).to(
device=device, dtype=torch.float32)
```

- SDv2-inpaint UNet ËæìÂÖ• [code](https://vscode.dev/github/Stability-AI/stablediffusion/blob/main/ldm/models/diffusion/ddpm.py#L1346)

Â∞Ü noise_fea(c=4), mask(c=1), masked_image_fea(c=4) ÂêàÂπ∂Ëµ∑Êù•ËæìÂÖ• UnetÔºå**Ë∞ÉÊï¥ unet ÁöÑ conv_in ÁöÑ C=9 ÈÄöÈÅì**

```
        elif self.conditioning_key == 'hybrid':
            xc = torch.cat([x] + c_concat, dim=1)
            cc = torch.cat(c_crossattn, 1)
            out = self.diffusion_model(xc, t, context=cc)
```

Framework ÂèÇËÄÉ BrushNet ÁöÑ Controlnet ÈÉ®ÂàÜÁöÑËæìÂÖ•

![ComparsionDiffusionFramework.png](docs/2024_03_Arxiv_BrushNet--A-Plug-and-Play-Image-Inpainting-Model-with-Decomposed-Dual-Branch-Diffusion_Note/ComparsionDiffusionFramework.png)





- QÔºöSDv2 github repo ‰ª£Á†ÅÂü∫‰∫é pytorch lightningÔºå‰∏çÊñπ‰æø

ÈáçÂÜô‰∏Ä‰∏™ diffuser ÁöÑÔºåÊâæ‰∏Ä‰∏™ diffusers SDv2 ÁöÑ‰ª£Á†Å :star:

> issue https://github.com/huggingface/diffusers/issues/1392#issuecomment-1326349638 >> https://huggingface.co/stabilityai/stable-diffusion-2-inpainting

```python
from diffusers import StableDiffusionInpaintPipeline
pipe = StableDiffusionInpaintPipeline.from_pretrained(
    "stabilityai/stable-diffusion-2-inpainting",
    torch_dtype=torch.float16,
)
pipe.to("cuda")
prompt = "Face of a yellow cat, high resolution, sitting on a park bench"
#image and mask_image should be PIL images.
#The mask structure is white for inpainting and black for keeping as is
image = pipe(prompt=prompt, image=image, mask_image=mask_image).images[0]
image.save("./yellow_cat_on_park_bench.png")
```



- QÔºöÊ≤°Êúâ controlnet or Lora Â¶Ç‰Ωï finetuneÔºü

> SDv2 inpaint finetune model [code](https://github.com/Stability-AI/stablediffusion/blob/cf1d67a6fd5ea1aa600c4df58e5b47da45f6bdbf/ldm/models/diffusion/ddpm.py#L1504)

Âè™ÂæÆË∞É‰∏Ä‰∏™ input block

```python
                 finetune_keys=("model.diffusion_model.input_blocks.0.0.weight",
                                "model_ema.diffusion_modelinput_blocks00weight"
                                ),
```



